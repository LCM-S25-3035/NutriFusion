{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo1Z_AYOtve-"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# ----- STEP 1: Nutrition lookup table (you can expand this) -----\n",
        "NUTRITION_TABLE = {\n",
        "    'chicken breast': {'calories': 165, 'fat': 3.6, 'protein': 31, 'fiber': 0},\n",
        "    'onion': {'calories': 40, 'fat': 0.1, 'protein': 1.1, 'fiber': 1.7},\n",
        "    'garlic': {'calories': 149, 'fat': 0.5, 'protein': 6.4, 'fiber': 2.1},\n",
        "    'butter': {'calories': 717, 'fat': 81, 'protein': 0.9, 'fiber': 0},\n",
        "    'rice': {'calories': 130, 'fat': 0.3, 'protein': 2.7, 'fiber': 0.4},\n",
        "    'milk': {'calories': 42, 'fat': 1, 'protein': 3.4, 'fiber': 0},\n",
        "    'salt': {'calories': 0, 'fat': 0, 'protein': 0, 'fiber': 0},\n",
        "    'pepper': {'calories': 251, 'fat': 3.3, 'protein': 10.4, 'fiber': 25.3},\n",
        "    'tomato': {'calories': 18, 'fat': 0.2, 'protein': 0.9, 'fiber': 1.2},\n",
        "    'carrot': {'calories': 41, 'fat': 0.2, 'protein': 0.9, 'fiber': 2.8}\n",
        "    # Add more ingredients as needed\n",
        "}\n",
        "\n",
        "# ----- STEP 2: API access functions -----\n",
        "def get_meals_by_category(category='Chicken'):\n",
        "    url = f\"https://www.themealdb.com/api/json/v1/1/filter.php?c={category}\"\n",
        "    response = requests.get(url)\n",
        "    return response.json().get('meals', [])\n",
        "\n",
        "def get_meal_details(meal_id):\n",
        "    url = f\"https://www.themealdb.com/api/json/v1/1/lookup.php?i={meal_id}\"\n",
        "    response = requests.get(url)\n",
        "    meals = response.json().get('meals', [])\n",
        "    return meals[0] if meals else {}\n",
        "\n",
        "# ----- STEP 3: Ingredient extraction -----\n",
        "def extract_ingredients(meal):\n",
        "    ingredients = {}\n",
        "    for i in range(1, 21):\n",
        "        ing = meal.get(f\"strIngredient{i}\")\n",
        "        meas = meal.get(f\"strMeasure{i}\")\n",
        "        if ing and ing.strip():\n",
        "            ingredients[ing.strip().lower()] = meas.strip() if meas else ''\n",
        "    return ingredients\n",
        "\n",
        "# ----- STEP 4: Nutrition estimation -----\n",
        "def estimate_nutrition(ingredients):\n",
        "    totals = {'calories': 0, 'fat': 0, 'protein': 0, 'fiber': 0}\n",
        "    for ing in ingredients:\n",
        "        for known in NUTRITION_TABLE:\n",
        "            if known in ing:  # fuzzy match\n",
        "                nut = NUTRITION_TABLE[known]\n",
        "                for key in totals:\n",
        "                    totals[key] += nut.get(key, 0)\n",
        "    return totals\n",
        "\n",
        "# ----- STEP 5: Main scraping + saving -----\n",
        "def scrape_meals(category='Chicken', limit=5):\n",
        "    meals = get_meals_by_category(category)\n",
        "    meal_data = []\n",
        "\n",
        "    for meal in meals[:limit]:\n",
        "        details = get_meal_details(meal['idMeal'])\n",
        "        ingredients = extract_ingredients(details)\n",
        "        nutrition = estimate_nutrition(ingredients.keys())\n",
        "\n",
        "        data = {\n",
        "            'MealID': meal['idMeal'],\n",
        "            'Meal': details.get('strMeal'),\n",
        "            'Category': details.get('strCategory'),\n",
        "            'Area': details.get('strArea'),\n",
        "            'Instructions': details.get('strInstructions'),\n",
        "            'Tags': details.get('strTags'),\n",
        "            'YouTube': details.get('strYoutube'),\n",
        "            'Ingredients': ingredients,\n",
        "            'Calories': nutrition['calories'],\n",
        "            'Fat': nutrition['fat'],\n",
        "            'Protein': nutrition['protein'],\n",
        "            'Fiber': nutrition['fiber'],\n",
        "        }\n",
        "        meal_data.append(data)\n",
        "\n",
        "    return pd.DataFrame(meal_data)\n",
        "\n",
        "# ----- STEP 6: Run and save -----\n",
        "if __name__ == \"__main__\":\n",
        "    df_recipes = scrape_meals(category='Chicken', limit=5)\n",
        "    df_recipes.to_csv(\"themealdb_recipes.csv\", index=False)\n",
        "    print(\"✅ Scraped and saved recipes with estimated nutrition to 'themealdb_recipes.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Simplified nutrient lookup table (approximate per 100g)\n",
        "NUTRITION_TABLE = {\n",
        "    'chicken': {'calories': 239, 'fat': 14, 'protein': 27, 'carbs': 0, 'sugar': 0, 'fiber': 0},\n",
        "    'rice': {'calories': 130, 'fat': 0.3, 'protein': 2.7, 'carbs': 28, 'sugar': 0.1, 'fiber': 0.4},\n",
        "    'onion': {'calories': 40, 'fat': 0.1, 'protein': 1.1, 'carbs': 9.3, 'sugar': 4.2, 'fiber': 1.7},\n",
        "    'garlic': {'calories': 149, 'fat': 0.5, 'protein': 6.4, 'carbs': 33, 'sugar': 1, 'fiber': 2.1},\n",
        "    'tomato': {'calories': 18, 'fat': 0.2, 'protein': 0.9, 'carbs': 3.9, 'sugar': 2.6, 'fiber': 1.2},\n",
        "    'milk': {'calories': 42, 'fat': 1, 'protein': 3.4, 'carbs': 5, 'sugar': 5, 'fiber': 0},\n",
        "    'butter': {'calories': 717, 'fat': 81, 'protein': 0.9, 'carbs': 0.1, 'sugar': 0.1, 'fiber': 0},\n",
        "    'carrot': {'calories': 41, 'fat': 0.2, 'protein': 0.9, 'carbs': 10, 'sugar': 4.7, 'fiber': 2.8},\n",
        "    'egg': {'calories': 155, 'fat': 11, 'protein': 13, 'carbs': 1.1, 'sugar': 1.1, 'fiber': 0},\n",
        "    'salt': {'calories': 0, 'fat': 0, 'protein': 0, 'carbs': 0, 'sugar': 0, 'fiber': 0}\n",
        "}\n",
        "\n",
        "def get_all_meal_ids():\n",
        "    meal_ids = set()\n",
        "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
        "        url = f'https://www.themealdb.com/api/json/v1/1/search.php?f={letter}'\n",
        "        res = requests.get(url)\n",
        "        data = res.json()\n",
        "        meals = data.get('meals', [])\n",
        "        if meals:  # ✅ Prevents TypeError when meals is None\n",
        "            for meal in meals:\n",
        "                meal_ids.add(meal['idMeal'])\n",
        "        time.sleep(0.3)\n",
        "    return list(meal_ids)\n",
        "\n",
        "def get_meal_details(meal_id):\n",
        "    url = f\"https://www.themealdb.com/api/json/v1/1/lookup.php?i={meal_id}\"\n",
        "    response = requests.get(url)\n",
        "    meals = response.json().get('meals', [])\n",
        "    return meals[0] if meals else None\n",
        "\n",
        "def extract_ingredients(meal):\n",
        "    ingredients = []\n",
        "    for i in range(1, 21):\n",
        "        ing = meal.get(f\"strIngredient{i}\")\n",
        "        if ing and ing.strip():\n",
        "            ingredients.append(ing.strip().lower())\n",
        "    return ingredients\n",
        "\n",
        "def estimate_nutrition(ingredients):\n",
        "    total = {'calories': 0, 'fat': 0, 'protein': 0, 'carbs': 0, 'sugar': 0, 'fiber': 0}\n",
        "    for ing in ingredients:\n",
        "        for known in NUTRITION_TABLE:\n",
        "            if known in ing:\n",
        "                for key in total:\n",
        "                    total[key] += NUTRITION_TABLE[known][key]\n",
        "    return total\n",
        "\n",
        "def scrape_meals(limit=500):\n",
        "    all_ids = get_all_meal_ids()\n",
        "    recipes = []\n",
        "    for meal_id in all_ids[:limit]:\n",
        "        meal = get_meal_details(meal_id)\n",
        "        if meal:\n",
        "            ingredients = extract_ingredients(meal)\n",
        "            nutrition = estimate_nutrition(ingredients)\n",
        "            recipes.append({\n",
        "                'MealID': meal_id,\n",
        "                'Meal': meal.get('strMeal'),\n",
        "                'Category': meal.get('strCategory'),\n",
        "                'Area': meal.get('strArea'),\n",
        "                'Instructions': meal.get('strInstructions'),\n",
        "                'Ingredients': ', '.join(ingredients),\n",
        "                'Calories': nutrition['calories'],\n",
        "                'Fat': nutrition['fat'],\n",
        "                'Protein': nutrition['protein'],\n",
        "                'Sugar': nutrition['sugar'],\n",
        "                'Fiber': nutrition['fiber'],\n",
        "                'Carbohydrates': nutrition['carbs']\n",
        "            })\n",
        "        time.sleep(0.2)\n",
        "    return pd.DataFrame(recipes)\n",
        "\n",
        "# Run the scraper\n",
        "df = scrape_meals(limit=500)\n",
        "df.to_csv(\"themealdb_500_recipes.csv\", index=False)\n",
        "print(\"✅ Done! Scraped 500 recipes from TheMealDB with estimated nutrition.\")"
      ],
      "metadata": {
        "id": "7l85-NSMuBl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# Nutritional lookup (approx. per 100g)\n",
        "NUTRITION_TABLE = {\n",
        "    'chicken': {'calories': 239, 'fat': 14, 'protein': 27, 'carbs': 0, 'sugar': 0, 'fiber': 0},\n",
        "    'rice': {'calories': 130, 'fat': 0.3, 'protein': 2.7, 'carbs': 28, 'sugar': 0.1, 'fiber': 0.4},\n",
        "    'onion': {'calories': 40, 'fat': 0.1, 'protein': 1.1, 'carbs': 9.3, 'sugar': 4.2, 'fiber': 1.7},\n",
        "    'garlic': {'calories': 149, 'fat': 0.5, 'protein': 6.4, 'carbs': 33, 'sugar': 1, 'fiber': 2.1},\n",
        "    'tomato': {'calories': 18, 'fat': 0.2, 'protein': 0.9, 'carbs': 3.9, 'sugar': 2.6, 'fiber': 1.2},\n",
        "    'milk': {'calories': 42, 'fat': 1, 'protein': 3.4, 'carbs': 5, 'sugar': 5, 'fiber': 0},\n",
        "    'butter': {'calories': 717, 'fat': 81, 'protein': 0.9, 'carbs': 0.1, 'sugar': 0.1, 'fiber': 0},\n",
        "    'carrot': {'calories': 41, 'fat': 0.2, 'protein': 0.9, 'carbs': 10, 'sugar': 4.7, 'fiber': 2.8},\n",
        "    'egg': {'calories': 155, 'fat': 11, 'protein': 13, 'carbs': 1.1, 'sugar': 1.1, 'fiber': 0},\n",
        "    'salt': {'calories': 0, 'fat': 0, 'protein': 0, 'carbs': 0, 'sugar': 0, 'fiber': 0}\n",
        "}\n",
        "\n",
        "# Headless browser setup for scraping\n",
        "options = Options()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "options.add_argument('--user-data-dir=/tmp/bbc-profile')  # avoids SessionNotCreatedException\n",
        "\n",
        "driver = webdriver.Chrome(options=options)\n",
        "\n",
        "def get_bbc_recipe_links(pages=50):\n",
        "    links = set()\n",
        "    for page in range(1, pages + 1):\n",
        "        print(f\"🔍 Fetching page {page}\")\n",
        "        url = f\"https://www.bbcgoodfood.com/recipes/category/all?page={page}\"\n",
        "        driver.get(url)\n",
        "        time.sleep(2)\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        for a in soup.select('h2.heading-4 a'):\n",
        "            href = a.get('href')\n",
        "            if href and '/recipes/' in href:\n",
        "                links.add(\"https://www.bbcgoodfood.com\" + href)\n",
        "    return list(links)\n",
        "\n",
        "def extract_ingredients_from_bbc(soup):\n",
        "    return [li.text.strip().lower() for li in soup.select('.ingredients-section li') if li.text.strip()]\n",
        "\n",
        "def estimate_nutrition(ingredients):\n",
        "    total = {'calories': 0, 'fat': 0, 'protein': 0, 'carbs': 0, 'sugar': 0, 'fiber': 0}\n",
        "    for ing in ingredients:\n",
        "        for known in NUTRITION_TABLE:\n",
        "            if known in ing:\n",
        "                for key in total:\n",
        "                    total[key] += NUTRITION_TABLE[known][key]\n",
        "    return total\n",
        "\n",
        "def get_bbc_recipe_details(url):\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(2)\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        title = soup.select_one('h1').text.strip()\n",
        "        ingredients = extract_ingredients_from_bbc(soup)\n",
        "        method = ' '.join([step.text.strip() for step in soup.select('.method__item')])\n",
        "        nutrition = estimate_nutrition(ingredients)\n",
        "        return {\n",
        "            'Meal': title,\n",
        "            'URL': url,\n",
        "            'Ingredients': ', '.join(ingredients),\n",
        "            'Instructions': method,\n",
        "            'Calories': nutrition['calories'],\n",
        "            'Fat': nutrition['fat'],\n",
        "            'Protein': nutrition['protein'],\n",
        "            'Sugar': nutrition['sugar'],\n",
        "            'Fiber': nutrition['fiber'],\n",
        "            'Carbohydrates': nutrition['carbs']\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def scrape_bbc_meals(limit=500):\n",
        "    links = get_bbc_recipe_links(pages=50)\n",
        "    print(f\"Found {len(links)} recipes.\")\n",
        "    recipes = []\n",
        "    for i, url in enumerate(links[:limit]):\n",
        "        print(f\"⏳ Scraping {i+1}/{limit}\")\n",
        "        meal = get_bbc_recipe_details(url)\n",
        "        if meal:\n",
        "            recipes.append(meal)\n",
        "        time.sleep(1)\n",
        "    return pd.DataFrame(recipes)\n",
        "\n",
        "# Run scraper\n",
        "df = scrape_bbc_meals(limit=500)\n",
        "df.to_csv(\"bbc_good_food_500_recipes.csv\", index=False)\n",
        "print(\"✅ Done! Scraped 500 BBC Good Food recipes with estimated nutrition.\")\n",
        "\n",
        "driver.quit()\n"
      ],
      "metadata": {
        "id": "zpAbq0fguKZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Simple nutrition lookup table (expand as needed)\n",
        "NUTRITION_TABLE = {\n",
        "    'chicken': {'calories': 239, 'fat': 14, 'protein': 27, 'carbs': 0, 'sugar': 0, 'fiber': 0},\n",
        "    'rice': {'calories': 130, 'fat': 0.3, 'protein': 2.7, 'carbs': 28, 'sugar': 0.1, 'fiber': 0.4},\n",
        "    'onion': {'calories': 40, 'fat': 0.1, 'protein': 1.1, 'carbs': 9.3, 'sugar': 4.2, 'fiber': 1.7},\n",
        "    'garlic': {'calories': 149, 'fat': 0.5, 'protein': 6.4, 'carbs': 33, 'sugar': 1, 'fiber': 2.1},\n",
        "    'tomato': {'calories': 18, 'fat': 0.2, 'protein': 0.9, 'carbs': 3.9, 'sugar': 2.6, 'fiber': 1.2},\n",
        "    'milk': {'calories': 42, 'fat': 1, 'protein': 3.4, 'carbs': 5, 'sugar': 5, 'fiber': 0},\n",
        "    'butter': {'calories': 717, 'fat': 81, 'protein': 0.9, 'carbs': 0.1, 'sugar': 0.1, 'fiber': 0},\n",
        "    'egg': {'calories': 155, 'fat': 11, 'protein': 13, 'carbs': 1.1, 'sugar': 1.1, 'fiber': 0},\n",
        "}\n",
        "\n",
        "def get_recipe_links(pages=25):\n",
        "    links = []\n",
        "    for i in range(1, pages + 1):\n",
        "        url = f\"https://www.allrecipes.com/recipes/?page={i}\"\n",
        "        res = requests.get(url)\n",
        "        soup = BeautifulSoup(res.content, 'html.parser')\n",
        "        for a in soup.select('a.card__titleLink'):\n",
        "            link = a.get('href')\n",
        "            if link and 'https://www.allrecipes.com/recipe/' in link:\n",
        "                links.append(link)\n",
        "        time.sleep(0.5)\n",
        "    return list(set(links))\n",
        "\n",
        "def extract_recipe(url):\n",
        "    res = requests.get(url)\n",
        "    soup = BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "    name = soup.find('h1', class_='headline').text.strip() if soup.find('h1', class_='headline') else 'N/A'\n",
        "    ingredients_tags = soup.select('span.ingredients-item-name')\n",
        "    ingredients = [i.text.strip().lower() for i in ingredients_tags if i.text.strip()]\n",
        "    instructions_tag = soup.select('ul.instructions-section li p')\n",
        "    instructions = ' '.join(i.text.strip() for i in instructions_tag)\n",
        "\n",
        "    nutrition = estimate_nutrition(ingredients)\n",
        "    return {\n",
        "        'Meal': name,\n",
        "        'Ingredients': ', '.join(ingredients),\n",
        "        'Instructions': instructions,\n",
        "        'Calories': nutrition['calories'],\n",
        "        'Fat': nutrition['fat'],\n",
        "        'Protein': nutrition['protein'],\n",
        "        'Sugar': nutrition['sugar'],\n",
        "        'Fiber': nutrition['fiber'],\n",
        "        'Carbohydrates': nutrition['carbs']\n",
        "    }\n",
        "\n",
        "def estimate_nutrition(ingredients):\n",
        "    total = {'calories': 0, 'fat': 0, 'protein': 0, 'carbs': 0, 'sugar': 0, 'fiber': 0}\n",
        "    for ing in ingredients:\n",
        "        for known in NUTRITION_TABLE:\n",
        "            if known in ing:\n",
        "                for key in total:\n",
        "                    total[key] += NUTRITION_TABLE[known][key]\n",
        "    return total\n",
        "\n",
        "def scrape_allrecipes(limit=500):\n",
        "    links = get_recipe_links(pages=40)\n",
        "    data = []\n",
        "    for i, link in enumerate(links[:limit]):\n",
        "        try:\n",
        "            print(f\"Scraping ({i+1}/{limit}): {link}\")\n",
        "            recipe = extract_recipe(link)\n",
        "            data.append(recipe)\n",
        "            time.sleep(0.4)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error scraping {link}: {e}\")\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# Scrape and Save\n",
        "df = scrape_allrecipes(limit=500)\n",
        "df.to_csv(\"allrecipes_500.csv\", index=False)\n",
        "print(\"✅ Done! Saved 500 recipes from AllRecipes.com.\")"
      ],
      "metadata": {
        "id": "VXLaGR4muXre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SQRbgkRpuYyx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}