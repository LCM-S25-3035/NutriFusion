{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oln0qsLyk1kr"
      },
      "outputs": [],
      "source": [
        "# cleanig code using spark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import trim, col\n",
        "\n",
        "def main():\n",
        "    spark = SparkSession.builder.appName(\"CleanDataset\").getOrCreate()\n",
        "\n",
        "    # Input and output paths - update file name if needed\n",
        "    parquet_output_path = \"gs://dataset_nutrifuson/cleaned-data/cleaned_dataset\"\n",
        "    csv_output_path = \"gs://dataset_nutrifusion/cleaned-data/csv_output\"\n",
        "    input_path = \"gs://dataset_nutrifusion/input-data/70000_recipes.csv\"\n",
        "\"\n",
        "\n",
        "    df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(input_path)\n",
        "\n",
        "\n",
        "    # Trim whitespace from string columns\n",
        "    string_cols = [f.name for f in df.schema.fields if f.dataType.simpleString() == \"string\"]\n",
        "    for c in string_cols:\n",
        "        df = df.withColumn(c, trim(col(c)))\n",
        "\n",
        "    # Drop rows with any nulls\n",
        "    df_clean = df.na.drop()\n",
        "\n",
        "    # Drop duplicates\n",
        "    df_clean = df_clean.dropDuplicates()\n",
        "\n",
        "    # Save cleaned dataset as Parquet\n",
        "    df_clean.write.mode(\"overwrite\").parquet(parquet_output_path)\n",
        "\n",
        "    # Also save cleaned dataset as CSV (with header)\n",
        "    df_clean.write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_output_path)\n",
        "\n",
        "    print(f\"Cleaned data saved as Parquet to: {parquet_output_path}\")\n",
        "    print(f\"Cleaned data saved as CSV to: {csv_output_path}\")\n",
        "\n",
        "    spark.stop()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1st created a project in gcs as Nutrifusion_project\n",
        "then created a bucket named dataset_nutrifusion\n"
      ],
      "metadata": {
        "id": "usYAEOrZRtyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# authenticate and onnect set the project using this\n",
        "gcloud auth login\n",
        "gcloud config set project nutrifusion-drive-2025\n"
      ],
      "metadata": {
        "id": "gnnhKBnzSRWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set the region\n",
        "gcloud config set dataproc/region us-central1\n"
      ],
      "metadata": {
        "id": "FEV6FwbbSYt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checked the cluster status\n",
        "gcloud dataproc clusters list\n",
        "#this was my specific cluster\n",
        "gcloud dataproc clusters describe recipe-cluster --region=us-central1\n",
        "\n"
      ],
      "metadata": {
        "id": "iNCdE5oLSkN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# uploaded dataset the cleanig code to gcs\n",
        "gsutil cp \"D:\\Downloads\\sparkjob_2.py\" gs://dataset_nutrifusion/scripts/\n"
      ],
      "metadata": {
        "id": "cBVFrGT3QDBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#uploaded the dataset to the gcs using sdk\n",
        "gsutil cp D:\\OneDrive\\Documents\\DSMM\\TERM 3\\capstone [project\\70000_recipes.csv gs://dataset_nutrifusion/input-data/"
      ],
      "metadata": {
        "id": "BjE4NXwERasO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# give me list of files in cluster\n",
        "gsutil ls gs://dataset_nutrifusion/cleaned-data/\n"
      ],
      "metadata": {
        "id": "wJh457ZIQL6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sparkjob in dataproc\n",
        "gcloud dataproc jobs submit pyspark D:\\Downloads\\sparkjob_2.py --cluster=recipe-cluster --region=us-central1\n",
        "Job [4c2fa67d5c664c50a9ff022de222e5a1] submitted."
      ],
      "metadata": {
        "id": "pK2YaK7-Q7vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code to convert the parquet to csv\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Step 1: Start Spark session\n",
        "spark = SparkSession.builder.appName(\"ParquetToCSV\").getOrCreate()\n",
        "\n",
        "# Step 2: Read Parquet data from GCS\n",
        "df = spark.read.parquet(\"gs://dataset_nutrifusion/cleaned-data/cleaned_dataset/\")\n",
        "\n",
        "# Step 3: Convert to CSV and save back to GCS (single file)\n",
        "df.coalesce(1).write.csv(\"gs://dataset_nutrifusion/cleaned-data/csv_output\", header=True, mode=\"overwrite\")"
      ],
      "metadata": {
        "id": "xx4HPANQRVvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parquet in dataproc\n",
        "gcloud dataproc jobs submit pyspark gs://dataset_nutrifusion/scripts/parquet.py --cluster=recipe-cluster --region=us-central1"
      ],
      "metadata": {
        "id": "Ef0s6L9QfMrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of datasets after runing the above\n",
        "gsutil ls gs://dataset_nutrifusion/cleaned-data/csv_output/"
      ],
      "metadata": {
        "id": "rho0Cj1ZgGOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code to download the csv file\n",
        "gsutil cp gs://dataset_nutrifusion/cleaned-data/csv_output/part-00000-d356466d-8296-43fd-8965-709379ee4473-c000.csv"
      ],
      "metadata": {
        "id": "87pi1Q46gM2C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}