{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "# Example recipe URL\n",
        "url = 'https://www.allrecipes.com/recipe/24074/alysias-basic-meat-lasagna/'\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "}\n",
        "\n",
        "def scrape_recipe(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find the title element and safely get the text\n",
        "    title_element = soup.find('h1', class_='headline heading-content')\n",
        "    # Check if title_element was found before calling get_text()\n",
        "    title = title_element.get_text(strip=True) if title_element else \"Title not found\"\n",
        "\n",
        "    # Ingredients\n",
        "    ingredients = [ing.get_text(strip=True) for ing in soup.select('span.ingredients-item-name')]\n",
        "\n",
        "    # Instructions\n",
        "    instructions = [step.get_text(strip=True) for step in soup.select('li.subcontainer.instructions-section-item')]\n",
        "\n",
        "    # Nutrition Info (if present)\n",
        "    nutrition_section = soup.find('div', class_='partial recipe-nutrition-section')\n",
        "    nutrition = nutrition_section.get_text(strip=True) if nutrition_section else \"Not available\"\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'ingredients': ingredients,\n",
        "        'instructions': instructions,\n",
        "        'nutrition': nutrition\n",
        "    }\n",
        "\n",
        "recipe_data = scrape_recipe(url)\n",
        "print(json.dumps(recipe_data, indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYxpDVsZOi3n",
        "outputId": "8856a507-5dac-44a5-b8ec-86f01c3a017d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"title\": \"Title not found\",\n",
            "  \"ingredients\": [],\n",
            "  \"instructions\": [],\n",
            "  \"nutrition\": \"Not available\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "def scrape_allrecipes_recipe(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "    }\n",
        "\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to retrieve page: Status code {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Recipe Title\n",
        "    title_tag = soup.find('h1', class_='headline heading-content')\n",
        "    title = title_tag.get_text(strip=True) if title_tag else 'N/A'\n",
        "\n",
        "    # Ingredients\n",
        "    ingredients = []\n",
        "    ingredient_tags = soup.select('span.ingredients-item-name')\n",
        "    for tag in ingredient_tags:\n",
        "        ingredients.append(tag.get_text(strip=True))\n",
        "\n",
        "    # Instructions\n",
        "    instructions = []\n",
        "    instruction_tags = soup.select('li.subcontainer.instructions-section-item div.section-body')\n",
        "    for tag in instruction_tags:\n",
        "        instructions.append(tag.get_text(strip=True))\n",
        "\n",
        "    # Nutrition\n",
        "    nutrition_tag = soup.find('div', class_='partial recipe-nutrition-section')\n",
        "    nutrition = nutrition_tag.get_text(strip=True) if nutrition_tag else 'N/A'\n",
        "\n",
        "    # Result Dictionary\n",
        "    recipe = {\n",
        "        'title': title,\n",
        "        'ingredients': ingredients,\n",
        "        'instructions': instructions,\n",
        "        'nutrition': nutrition\n",
        "    }\n",
        "\n",
        "    return recipe\n",
        "\n",
        "# Test with a sample AllRecipes URL\n",
        "recipe_url = 'https://www.allrecipes.com/recipe/24074/alysias-basic-meat-lasagna/'\n",
        "data = scrape_allrecipes_recipe(recipe_url)\n",
        "\n",
        "# Output as JSON\n",
        "print(json.dumps(data, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkW7GWQSPHrr",
        "outputId": "5b3364e0-2b9c-4de9-d474-44c7cae73cb8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"title\": \"N/A\",\n",
            "  \"ingredients\": [],\n",
            "  \"instructions\": [],\n",
            "  \"nutrition\": \"N/A\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkHmHfFVPq1l",
        "outputId": "6a6771d5-9814-4be1-ddd9-7fb074854dd5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "}\n",
        "\n",
        "def get_recipe_links(search_term, max_results=5):\n",
        "    search_url = f'https://www.allrecipes.com/search/results/?search={search_term}'\n",
        "    response = requests.get(search_url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    recipe_links = []\n",
        "    for link_tag in soup.select('a.card__titleLink'):\n",
        "        href = link_tag.get('href')\n",
        "        if href and href.startswith('https://www.allrecipes.com/recipe/'):\n",
        "            recipe_links.append(href)\n",
        "        if len(recipe_links) >= max_results:\n",
        "            break\n",
        "    return recipe_links\n",
        "\n",
        "def scrape_recipe(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    title_tag = soup.find('h1', class_='headline heading-content')\n",
        "    title = title_tag.get_text(strip=True) if title_tag else 'N/A'\n",
        "\n",
        "    ingredients = [tag.get_text(strip=True) for tag in soup.select('span.ingredients-item-name')]\n",
        "\n",
        "    instructions = [tag.get_text(strip=True)\n",
        "                    for tag in soup.select('li.subcontainer.instructions-section-item div.section-body')]\n",
        "\n",
        "    nutrition_tag = soup.find('div', class_='partial recipe-nutrition-section')\n",
        "    nutrition = nutrition_tag.get_text(strip=True) if nutrition_tag else 'N/A'\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'url': url,\n",
        "        'ingredients': ingredients,\n",
        "        'instructions': instructions,\n",
        "        'nutrition': nutrition\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    search_term = \"chicken\"\n",
        "    recipe_urls = get_recipe_links(search_term, max_results=5)\n",
        "\n",
        "    all_recipes = []\n",
        "    for url in recipe_urls:\n",
        "        print(f\"Scraping: {url}\")\n",
        "        recipe_data = scrape_recipe(url)\n",
        "        all_recipes.append(recipe_data)\n",
        "        time.sleep(2)  # be polite with a delay\n",
        "\n",
        "    # Save or print result\n",
        "    print(json.dumps(all_recipes, indent=2))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xOYQp6nPtRa",
        "outputId": "fc3e0cd5-fac6-4f0e-fc06-e7c3ac557e43"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "}\n",
        "\n",
        "def get_recipe_links(search_term, max_results=5):\n",
        "    search_url = f'https://www.allrecipes.com/search/results/?search={search_term}'\n",
        "    response = requests.get(search_url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    recipe_links = []\n",
        "    for link_tag in soup.select('a.card__titleLink'):\n",
        "        href = link_tag.get('href')\n",
        "        if href and href.startswith('https://www.allrecipes.com/recipe/'):\n",
        "            recipe_links.append(href)\n",
        "        if len(recipe_links) >= max_results:\n",
        "            break\n",
        "    return recipe_links\n",
        "\n",
        "def scrape_recipe(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    title_tag = soup.find('h1', class_='headline heading-content')\n",
        "    title = title_tag.get_text(strip=True) if title_tag else 'N/A'\n",
        "\n",
        "    ingredients = [tag.get_text(strip=True) for tag in soup.select('span.ingredients-item-name')]\n",
        "    instructions = [tag.get_text(strip=True)\n",
        "                    for tag in soup.select('li.subcontainer.instructions-section-item div.section-body')]\n",
        "\n",
        "    nutrition_tag = soup.find('div', class_='partial recipe-nutrition-section')\n",
        "    nutrition = nutrition_tag.get_text(strip=True) if nutrition_tag else 'N/A'\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'url': url,\n",
        "        'ingredients': '; '.join(ingredients),\n",
        "        'instructions': ' '.join(instructions),\n",
        "        'nutrition': nutrition\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    search_term = \"chicken\"\n",
        "    recipe_urls = get_recipe_links(search_term, max_results=5)\n",
        "\n",
        "    all_recipes = []\n",
        "    for url in recipe_urls:\n",
        "        print(f\"Scraping: {url}\")\n",
        "        recipe_data = scrape_recipe(url)\n",
        "        all_recipes.append(recipe_data)\n",
        "        time.sleep(2)  # Be polite\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(all_recipes)\n",
        "\n",
        "    # Save to CSV and Excel\n",
        "    df.to_csv('chicken_recipes.csv', index=False)\n",
        "    df.to_excel('chicken_recipes.xlsx', index=False)\n",
        "\n",
        "    print(\"✅ Data saved to 'chicken_recipes.csv' and 'chicken_recipes.xlsx'.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlcC_eeBQGQb",
        "outputId": "afd8b949-0564-4235-ea03-afdd4dd32c76"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data saved to 'chicken_recipes.csv' and 'chicken_recipes.xlsx'.\n"
          ]
        }
      ]
    }
  ]
}