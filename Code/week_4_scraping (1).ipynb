{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kExc1TXHuhno",
        "outputId": "236e1884-4f09-4823-e6df-4507e5a2836a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Found 0 recipes on page 1.\n",
            "Scraping page 2...\n",
            "Found 0 recipes on page 2.\n",
            "Saved 0 recipes to serious_eats_recipes.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Function to fetch and parse a recipe page\n",
        "def get_recipe_details(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    try:\n",
        "        title = soup.find('h1').get_text(strip=True)\n",
        "    except AttributeError:\n",
        "        title = 'N/A'\n",
        "\n",
        "    ingredients = []\n",
        "    for li in soup.select(\"ul.ingredient-list li\"):\n",
        "        ingredients.append(li.get_text(strip=True))\n",
        "\n",
        "    instructions = []\n",
        "    for step in soup.select(\"ol.recipe-procedures li\"):\n",
        "        step_text = step.get_text(strip=True)\n",
        "        if step_text:\n",
        "            instructions.append(step_text)\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'ingredients': ingredients,\n",
        "        'instructions': instructions\n",
        "    }\n",
        "\n",
        "# Function to scrape multiple recipes from the Serious Eats listing pages\n",
        "def scrape_serious_eats_recipes(base_url, num_pages=1):\n",
        "    recipes = []\n",
        "\n",
        "    for page in range(1, num_pages + 1):\n",
        "        print(f\"Scraping page {page}...\")\n",
        "        url = f\"{base_url}?page={page}\"\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Find recipe links\n",
        "        recipe_links = []\n",
        "        for a in soup.select('a[href^=\"https://www.seriouseats.com/\"][data-vars-gtm-click-type=\"recipe\"]'):\n",
        "            link = a['href']\n",
        "            if link not in recipe_links:\n",
        "                recipe_links.append(link)\n",
        "\n",
        "        print(f\"Found {len(recipe_links)} recipes on page {page}.\")\n",
        "\n",
        "        for link in recipe_links:\n",
        "            try:\n",
        "                print(f\"Fetching recipe: {link}\")\n",
        "                details = get_recipe_details(link)\n",
        "                recipes.append(details)\n",
        "                time.sleep(1)  # be polite to the server\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to fetch {link}: {e}\")\n",
        "\n",
        "    return recipes\n",
        "\n",
        "# Save recipes to CSV\n",
        "def save_recipes_to_csv(recipes, filename='serious_eats_recipes.csv'):\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow(['Title', 'Ingredients', 'Instructions'])\n",
        "\n",
        "        for recipe in recipes:\n",
        "            writer.writerow([\n",
        "                recipe['title'],\n",
        "                '; '.join(recipe['ingredients']),\n",
        "                ' '.join(recipe['instructions'])\n",
        "            ])\n",
        "    print(f\"Saved {len(recipes)} recipes to {filename}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    BASE_URL = \"https://www.seriouseats.com/recipes\"\n",
        "    recipes = scrape_serious_eats_recipes(BASE_URL, num_pages=2)\n",
        "    save_recipes_to_csv(recipes)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# List of sample recipe URLs\n",
        "recipe_urls = [\n",
        "    \"https://www.seriouseats.com/best-chocolate-chip-cookies-recipe\",\n",
        "    \"https://www.seriouseats.com/classic-macaroni-and-cheese-recipe\",\n",
        "    \"https://www.seriouseats.com/easy-chicken-curry-recipe\"\n",
        "]\n",
        "\n",
        "# Function to get recipe data\n",
        "def get_recipe_data(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    res = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "    # Title\n",
        "    title_tag = soup.find(\"h1\")\n",
        "    title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
        "\n",
        "    # Ingredients\n",
        "    ingredients = []\n",
        "    ingredient_tags = soup.select(\"ul.structured-ingredients__list li\")\n",
        "    if not ingredient_tags:  # fallback for older structure\n",
        "        ingredient_tags = soup.select(\"li.ingredient\")\n",
        "    for li in ingredient_tags:\n",
        "        ingredients.append(li.get_text(strip=True))\n",
        "\n",
        "    # Instructions\n",
        "    instructions = []\n",
        "    step_tags = soup.select(\"ol.comp.mntl-sc-block-group--OL li\")\n",
        "    if not step_tags:  # fallback\n",
        "        step_tags = soup.select(\"div.section-body p\")\n",
        "    for step in step_tags:\n",
        "        text = step.get_text(strip=True)\n",
        "        if text:\n",
        "            instructions.append(text)\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'ingredients': ingredients,\n",
        "        'instructions': instructions\n",
        "    }\n",
        "\n",
        "# Save to CSV\n",
        "def save_to_csv(data, filename=\"serious_eats_recipes.csv\"):\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Title\", \"Ingredients\", \"Instructions\"])\n",
        "        for recipe in data:\n",
        "            writer.writerow([\n",
        "                recipe['title'],\n",
        "                \"; \".join(recipe['ingredients']),\n",
        "                \" \".join(recipe['instructions'])\n",
        "            ])\n",
        "    print(f\"✅ Saved {len(data)} recipes to {filename}\")\n",
        "\n",
        "# Main runner\n",
        "if __name__ == \"__main__\":\n",
        "    all_recipes = []\n",
        "    for url in recipe_urls:\n",
        "        print(f\"🔎 Scraping: {url}\")\n",
        "        try:\n",
        "            recipe = get_recipe_data(url)\n",
        "            all_recipes.append(recipe)\n",
        "            time.sleep(1)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error scraping {url}: {e}\")\n",
        "\n",
        "    save_to_csv(all_recipes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Egy_jNkyuk28",
        "outputId": "8675955f-75d7-47a8-cac7-6ffd9e09602d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Scraping: https://www.seriouseats.com/best-chocolate-chip-cookies-recipe\n",
            "🔎 Scraping: https://www.seriouseats.com/classic-macaroni-and-cheese-recipe\n",
            "🔎 Scraping: https://www.seriouseats.com/easy-chicken-curry-recipe\n",
            "✅ Saved 3 recipes to serious_eats_recipes.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install selenium beautifulsoup4 pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lp4pprXqvMoW",
        "outputId": "1e120ce3-50e3-40fd-bbb8-1e91fcfdbff3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.33.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: urllib3~=2.4.0 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (2.4.0)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: typing_extensions~=4.13.2 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.33.0 trio-0.30.0 trio-websocket-0.12.2 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# --- Setup Selenium ---\n",
        "def get_driver():\n",
        "    options = Options()\n",
        "    options.add_argument(\"--headless\")  # run browser in background\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    return driver\n",
        "\n",
        "# --- Step 1: Collect Recipe URLs from listing pages ---\n",
        "def get_recipe_urls(max_links=500):\n",
        "    print(\"🔍 Collecting recipe URLs...\")\n",
        "    driver = get_driver()\n",
        "    url = \"https://www.seriouseats.com/recipes\"\n",
        "    driver.get(url)\n",
        "\n",
        "    # Scroll to load more content dynamically\n",
        "    SCROLL_PAUSE_TIME = 2\n",
        "    links = set()\n",
        "    scrolls = 0\n",
        "\n",
        "    while len(links) < max_links and scrolls < 100:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(SCROLL_PAUSE_TIME)\n",
        "\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "        recipe_cards = soup.select('a[data-vars-gtm-click-type=\"recipe\"]')\n",
        "\n",
        "        for card in recipe_cards:\n",
        "            href = card.get(\"href\")\n",
        "            if href and href.startswith(\"https://www.seriouseats.com/\") and \"/recipe\" in href:\n",
        "                links.add(href)\n",
        "\n",
        "        scrolls += 1\n",
        "        print(f\"Collected {len(links)} links...\")\n",
        "\n",
        "    driver.quit()\n",
        "    return list(links)[:max_links]\n",
        "\n",
        "# --- Step 2: Scrape Recipe Content ---\n",
        "def scrape_recipe(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    try:\n",
        "        res = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "        title = soup.find(\"h1\").get_text(strip=True)\n",
        "\n",
        "        ingredients = [li.get_text(strip=True) for li in soup.select(\"ul.structured-ingredients__list li\")]\n",
        "        if not ingredients:\n",
        "            ingredients = [li.get_text(strip=True) for li in soup.select(\"li.ingredient\")]\n",
        "\n",
        "        steps = [step.get_text(strip=True) for step in soup.select(\"ol.comp.mntl-sc-block-group--OL li\")]\n",
        "        if not steps:\n",
        "            steps = [p.get_text(strip=True) for p in soup.select(\"div.section-body p\")]\n",
        "\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"ingredients\": \"; \".join(ingredients),\n",
        "            \"instructions\": \" \".join(steps),\n",
        "            \"url\": url\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Step 3: Save to CSV ---\n",
        "def save_to_csv(recipes, filename=\"serious_eats_500_recipes.csv\"):\n",
        "    df = pd.DataFrame(recipes)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"✅ Saved {len(recipes)} recipes to {filename}\")\n",
        "\n",
        "# --- Main Runner ---\n",
        "if __name__ == \"__main__\":\n",
        "    import requests\n",
        "    all_urls = get_recipe_urls(max_links=500)\n",
        "    all_recipes = []\n",
        "\n",
        "    for i, url in enumerate(all_urls):\n",
        "        print(f\"📄 Scraping {i+1}/{len(all_urls)}: {url}\")\n",
        "        data = scrape_recipe(url)\n",
        "        if data:\n",
        "            all_recipes.append(data)\n",
        "        time.sleep(1)  # polite delay\n",
        "\n",
        "    save_to_csv(all_recipes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gz-oOzmMvYqe",
        "outputId": "010a1b57-5812-4714-cb6c-eb2f97bbd335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Collecting recipe URLs...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "Collected 0 links...\n",
            "✅ Saved 0 recipes to serious_eats_500_recipes.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# --- Set up Selenium headless browser ---\n",
        "def get_driver():\n",
        "    options = Options()\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    return driver\n",
        "\n",
        "# --- Step 1: Scrape recipe URLs from listing page ---\n",
        "def get_recipe_urls(target_count=200):\n",
        "    print(\"🔍 Collecting recipe URLs...\")\n",
        "    driver = get_driver()\n",
        "    driver.get(\"https://www.seriouseats.com/recipes\")\n",
        "    time.sleep(3)\n",
        "\n",
        "    recipe_links = set()\n",
        "    scrolls = 0\n",
        "    max_scrolls = 100\n",
        "\n",
        "    while len(recipe_links) < target_count and scrolls < max_scrolls:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(2)\n",
        "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "        anchors = soup.select('a[data-vars-gtm-click-type=\"recipe\"]')\n",
        "\n",
        "        for a in anchors:\n",
        "            href = a.get(\"href\")\n",
        "            if href and href.startswith(\"https://www.seriouseats.com/\") and \"/recipe\" in href:\n",
        "                recipe_links.add(href)\n",
        "\n",
        "        scrolls += 1\n",
        "        print(f\"📦 {len(recipe_links)} recipes found...\")\n",
        "\n",
        "    driver.quit()\n",
        "    return list(recipe_links)[:target_count]\n",
        "\n",
        "# --- Step 2: Scrape individual recipe data ---\n",
        "def get_recipe_data(url):\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    res = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "    # Title\n",
        "    title_tag = soup.find(\"h1\")\n",
        "    title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
        "\n",
        "    # Ingredients\n",
        "    ingredients = []\n",
        "    ingredient_tags = soup.select(\"ul.structured-ingredients__list li\")\n",
        "    if not ingredient_tags:\n",
        "        ingredient_tags = soup.select(\"li.ingredient\")\n",
        "    for li in ingredient_tags:\n",
        "        ingredients.append(li.get_text(strip=True))\n",
        "\n",
        "    # Instructions\n",
        "    instructions = []\n",
        "    step_tags = soup.select(\"ol.comp.mntl-sc-block-group--OL li\")\n",
        "    if not step_tags:\n",
        "        step_tags = soup.select(\"div.section-body p\")\n",
        "    for step in step_tags:\n",
        "        text = step.get_text(strip=True)\n",
        "        if text:\n",
        "            instructions.append(text)\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'ingredients': ingredients,\n",
        "        'instructions': instructions\n",
        "    }\n",
        "\n",
        "# --- Step 3: Save all recipes to CSV ---\n",
        "def save_to_csv(data, filename=\"serious_eats_200_recipes.csv\"):\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"Title\", \"Ingredients\", \"Instructions\"])\n",
        "        for recipe in data:\n",
        "            writer.writerow([\n",
        "                recipe['title'],\n",
        "                \"; \".join(recipe['ingredients']),\n",
        "                \" \".join(recipe['instructions'])\n",
        "            ])\n",
        "    print(f\"✅ Saved {len(data)} recipes to {filename}\")\n",
        "\n",
        "# --- Main function ---\n",
        "if __name__ == \"__main__\":\n",
        "    recipe_urls = get_recipe_urls(target_count=200)\n",
        "\n",
        "    all_recipes = []\n",
        "    for i, url in enumerate(recipe_urls):\n",
        "        print(f\"🔎 Scraping {i+1}/{len(recipe_urls)}: {url}\")\n",
        "        try:\n",
        "            recipe = get_recipe_data(url)\n",
        "            all_recipes.append(recipe)\n",
        "            time.sleep(1)\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error scraping {url}: {e}\")\n",
        "\n",
        "    save_to_csv(all_recipes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a2X09MTvdeH",
        "outputId": "7394938d-ecc0-4b10-eeab-f0d8d3565b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Collecting recipe URLs...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "📦 0 recipes found...\n",
            "✅ Saved 0 recipes to serious_eats_200_recipes.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4 pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXu440mXwWiW",
        "outputId": "f7725a62-db95-46af-86f1-6e01d08cf696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def get_recipe_links(page_url):\n",
        "    res = requests.get(page_url)\n",
        "    soup = BeautifulSoup(res.content, 'html.parser')\n",
        "    links = []\n",
        "    for a in soup.select('a.card__titleLink'):\n",
        "        link = a.get('href')\n",
        "        if link and link.startswith(\"https://www.allrecipes.com/recipe/\"):\n",
        "            links.append(link)\n",
        "    return list(set(links))\n",
        "\n",
        "def get_recipe_data(recipe_url):\n",
        "    res = requests.get(recipe_url)\n",
        "    soup = BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "    title = soup.find('h1', class_='headline heading-content')\n",
        "    title = title.text.strip() if title else 'N/A'\n",
        "\n",
        "    ingredients = [i.text.strip() for i in soup.select('span.ingredients-item-name')]\n",
        "    ingredients = ', '.join(ingredients)\n",
        "\n",
        "    nutrition = soup.find('div', class_='partial recipe-nutrition-section')\n",
        "    nutrition = nutrition.get_text(strip=True) if nutrition else 'N/A'\n",
        "\n",
        "    return {\n",
        "        'Title': title,\n",
        "        'Ingredients': ingredients,\n",
        "        'Nutrition': nutrition,\n",
        "        'URL': recipe_url\n",
        "    }\n",
        "\n",
        "# Scrape multiple pages\n",
        "all_data = []\n",
        "base_url = 'https://www.allrecipes.com/recipes/?page='\n",
        "for page in range(1, 3):  # Scrape first 2 pages (increase if needed)\n",
        "    print(f\"Scraping page {page}...\")\n",
        "    recipe_links = get_recipe_links(base_url + str(page))\n",
        "    for link in recipe_links:\n",
        "        try:\n",
        "            data = get_recipe_data(link)\n",
        "            all_data.append(data)\n",
        "            print(f\"Scraped: {data['Title']}\")\n",
        "            time.sleep(1)  # Be polite\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {link}: {e}\")\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(all_data)\n",
        "df.to_csv('nutrifusion_recipes.csv', index=False)\n",
        "print(\"✅ Data saved to nutrifusion_recipes.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5hGnxZNx-UZ",
        "outputId": "a6a0f125-6254-409d-f814-bd888375c526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "✅ Data saved to nutrifusion_recipes.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from requests.exceptions import ChunkedEncodingError, ConnectionError, Timeout\n",
        "\n",
        "# Function to get recipe links from a search results page\n",
        "def get_recipe_links(page_url):\n",
        "    try:\n",
        "        response = requests.get(page_url, timeout=10) # Added timeout\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        links = []\n",
        "        for a_tag in soup.select('a.card__titleLink'):\n",
        "            link = a_tag.get('href')\n",
        "            if link and link.startswith(\"https://www.allrecipes.com/recipe/\"):\n",
        "                links.append(link)\n",
        "        return list(set(links))\n",
        "    except (ChunkedEncodingError, ConnectionError, Timeout) as e:\n",
        "        print(f\"⚠️ Error fetching page {page_url}: {e}\")\n",
        "        return [] # Return empty list if there's an error\n",
        "\n",
        "# Function to extract data from each recipe page\n",
        "def get_recipe_data(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10) # Added timeout\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        title = soup.find('h1', class_='headline heading-content')\n",
        "        title = title.text.strip() if title else \"N/A\"\n",
        "\n",
        "        ingredients = [i.text.strip() for i in soup.select('span.ingredients-item-name')]\n",
        "        ingredients = ', '.join(ingredients) if ingredients else \"N/A\"\n",
        "\n",
        "        nutrition_section = soup.find('div', class_='partial recipe-nutrition-section')\n",
        "        nutrition = nutrition_section.get_text(strip=True).replace(\"Full Nutrition\", \"\") if nutrition_section else \"N/A\"\n",
        "\n",
        "        return {\n",
        "            \"Title\": title,\n",
        "            \"Ingredients\": ingredients,\n",
        "            \"Nutrition\": nutrition,\n",
        "            \"URL\": url\n",
        "        }\n",
        "    except (ChunkedEncodingError, ConnectionError, Timeout) as e:\n",
        "        print(f\"⚠️ Error scraping recipe {url}: {e}\")\n",
        "        return None # Return None if there's an error\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ An unexpected error occurred while scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Main loop to collect at least 300 recipes\n",
        "all_recipes = []\n",
        "seen_links = set()\n",
        "page = 1\n",
        "\n",
        "print(\"⏳ Starting to scrape recipes...\")\n",
        "\n",
        "# Continue scraping from the page where it left off\n",
        "start_page = page # Use the last successfully scraped page as the starting point\n",
        "\n",
        "while len(all_recipes) < 300:\n",
        "    page_url = f'https://www.allrecipes.com/recipes/?page={start_page}'\n",
        "    print(f\"🔍 Scraping page {start_page}...\")\n",
        "    recipe_links = get_recipe_links(page_url)\n",
        "\n",
        "    if not recipe_links: # If no links are found on a page, it might be the end or an error.\n",
        "        print(f\"🛑 No new links found on page {start_page}. Ending scraping.\")\n",
        "        break\n",
        "\n",
        "    for link in tqdm(recipe_links, desc=f\"Processing page {start_page}\", leave=False):\n",
        "        if link not in seen_links:\n",
        "            try:\n",
        "                recipe = get_recipe_data(link)\n",
        "                if recipe and recipe['Title'] != \"N/A\" and recipe['Ingredients'] != \"N/A\" and recipe['Nutrition'] != \"N/A\":\n",
        "                    all_recipes.append(recipe)\n",
        "                    seen_links.add(link)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error with {link}: {e}\")\n",
        "            time.sleep(2)  # Increased delay to be more polite\n",
        "\n",
        "        if len(all_recipes) >= 300:\n",
        "            break\n",
        "    start_page += 1 # Increment page number for the next iteration\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(all_recipes)\n",
        "df.to_csv(\"nutrifusion_300_recipes.csv\", index=False)\n",
        "print(f\"✅ Scraping complete! {len(all_recipes)} recipes saved to 'nutrifusion_300_recipes.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0PMJ1LcyBv5",
        "outputId": "59c55f0d-115a-4a92-e029-df8a8d8b2f78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ Starting to scrape recipes...\n",
            "🔍 Scraping page 1...\n",
            "🛑 No new links found on page 1. Ending scraping.\n",
            "✅ Scraping complete! 0 recipes saved to 'nutrifusion_300_recipes.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wfZCMmZEyjAy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}