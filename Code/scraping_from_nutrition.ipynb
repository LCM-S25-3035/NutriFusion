{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxBpj9CcNy1F",
        "outputId": "2f1f8462-d628-4461-c6cf-4db8fd9d6a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping complete. Saved to 'nutrition_gov_recipes.csv'\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Base URL for recipes\n",
        "base_url = \"https://www.nutrition.gov/topics/whats-food/recipes\"\n",
        "\n",
        "# Headers to mimic a real browser\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "# Function to extract individual recipe links from the main page\n",
        "def get_recipe_links(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all links that lead to recipe detail pages\n",
        "    recipe_links = []\n",
        "    for a in soup.find_all('a', href=True):\n",
        "        href = a['href']\n",
        "        if '/recipes/' in href and href.startswith('/recipes/'):\n",
        "            full_link = \"https://www.nutrition.gov\" + href\n",
        "            if full_link not in recipe_links:\n",
        "                recipe_links.append(full_link)\n",
        "    return recipe_links\n",
        "\n",
        "# Function to parse a recipe page\n",
        "def parse_recipe(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    title = soup.find('h1').get_text(strip=True) if soup.find('h1') else \"No title\"\n",
        "\n",
        "    ingredients = soup.find('div', class_='field--name-field-ingredients')\n",
        "    ingredients_text = ingredients.get_text(separator='; ', strip=True) if ingredients else \"Not available\"\n",
        "\n",
        "    directions = soup.find('div', class_='field--name-field-directions')\n",
        "    directions_text = directions.get_text(separator=' ', strip=True) if directions else \"Not available\"\n",
        "\n",
        "    nutrition_info = soup.find('div', class_='field--name-field-nutrition')\n",
        "    nutrition_text = nutrition_info.get_text(separator=' ', strip=True) if nutrition_info else \"Not available\"\n",
        "\n",
        "    return {\n",
        "        \"Title\": title,\n",
        "        \"Ingredients\": ingredients_text,\n",
        "        \"Directions\": directions_text,\n",
        "        \"Nutrition Info\": nutrition_text,\n",
        "        \"URL\": url\n",
        "    }\n",
        "\n",
        "# Scrape the first page of recipes\n",
        "recipe_links = get_recipe_links(base_url)\n",
        "\n",
        "# Scrape each recipe\n",
        "data = []\n",
        "for link in recipe_links[:10]:  # Limit to 10 for demo purposes\n",
        "    print(f\"Scraping: {link}\")\n",
        "    recipe_data = parse_recipe(link)\n",
        "    data.append(recipe_data)\n",
        "    time.sleep(1)  # Be polite: avoid hammering the server\n",
        "\n",
        "# Save to DataFrame\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Base URL for recipes\n",
        "base_url = \"https://www.nutrition.gov/topics/whats-food/recipes\"\n",
        "\n",
        "# Headers to mimic a real browser\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "# Function to extract individual recipe links from the main page\n",
        "def get_recipe_links(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all links that lead to recipe detail pages\n",
        "    recipe_links = []\n",
        "    for a in soup.find_all('a', href=True):\n",
        "        href = a['href']\n",
        "        if '/recipes/' in href and href.startswith('/recipes/'):\n",
        "            full_link = \"https://www.nutrition.gov\" + href\n",
        "            if full_link not in recipe_links:\n",
        "                recipe_links.append(full_link)\n",
        "    return recipe_links\n",
        "\n",
        "# Function to parse a recipe page\n",
        "def parse_recipe(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    title = soup.find('h1').get_text(strip=True) if soup.find('h1') else \"No title\"\n",
        "\n",
        "    ingredients = soup.find('div', class_='field--name-field-ingredients')\n",
        "    ingredients_text = ingredients.get_text(separator='; ', strip=True) if ingredients else \"Not available\"\n",
        "\n",
        "    directions = soup.find('div', class_='field--name-field-directions')\n",
        "    directions_text = directions.get_text(separator=' ', strip=True) if directions else \"Not available\"\n",
        "\n",
        "    nutrition_info = soup.find('div', class_='field--name-field-nutrition')\n",
        "    nutrition_text = nutrition_info.get_text(separator=' ', strip=True) if nutrition_info else \"Not available\"\n",
        "\n",
        "    return {\n",
        "        \"Title\": title,\n",
        "        \"Ingredients\": ingredients_text,\n",
        "        \"Directions\": directions_text,\n",
        "        \"Nutrition Info\": nutrition_text,\n",
        "        \"URL\": url\n",
        "    }\n",
        "\n",
        "# Scrape the first page of recipes\n",
        "recipe_links = get_recipe_links(base_url)\n",
        "\n",
        "# Scrape each recipe\n",
        "data = []\n",
        "for link in recipe_links[:10]:  # Limit to 10 for demo purposes\n",
        "    print(f\"Scraping: {link}\")\n",
        "    recipe_data = parse_recipe(link)\n",
        "    data.append(recipe_data)\n",
        "    time.sleep(1)  # Be polite: avoid hammering the server\n",
        "\n",
        "# Save to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(\"nutrition_gov_recipes.csv\", index=False)\n",
        "print(\"Scraping complete. Saved to 'nutrition_gov_recipes.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "base_url = \"https://www.nutrition.gov/topics/whats-food/recipes\"\n",
        "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "\n",
        "def get_recipe_links(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    recipe_links = []\n",
        "    for a in soup.find_all('a', href=True):\n",
        "        href = a['href']\n",
        "        if '/recipes/' in href and href.startswith('/recipes/'):\n",
        "            full_link = \"https://www.nutrition.gov\" + href\n",
        "            if full_link not in recipe_links:\n",
        "                recipe_links.append(full_link)\n",
        "    return recipe_links\n",
        "\n",
        "def parse_recipe(url):\n",
        "    response = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    title = soup.find('h1').get_text(strip=True) if soup.find('h1') else \"No title\"\n",
        "    ingredients = soup.find('div', class_='field--name-field-ingredients')\n",
        "    directions = soup.find('div', class_='field--name-field-directions')\n",
        "    nutrition_info = soup.find('div', class_='field--name-field-nutrition')\n",
        "    return {\n",
        "        \"Title\": title,\n",
        "        \"Ingredients\": ingredients.get_text(separator='; ', strip=True) if ingredients else \"Not available\",\n",
        "        \"Directions\": directions.get_text(separator=' ', strip=True) if directions else \"Not available\",\n",
        "        \"Nutrition Info\": nutrition_info.get_text(separator=' ', strip=True) if nutrition_info else \"Not available\",\n",
        "        \"URL\": url\n",
        "    }\n",
        "\n",
        "# Scrape recipe links and data\n",
        "recipe_links = get_recipe_links(base_url)\n",
        "data = [parse_recipe(link) for link in recipe_links[:10]]  # Limit to first 10\n",
        "time.sleep(1)\n",
        "\n",
        "# Convert to DataFrame and save only if not empty\n",
        "df = pd.DataFrame(data)\n",
        "if not df.empty:\n",
        "    df.to_csv(\"nutrition_gov_recipes.csv\", index=False)\n",
        "    print(\"✅ Saved to 'nutrition_gov_recipes.csv'\")\n",
        "else:\n",
        "    print(\"❌ No data scraped. Please check the site structure or your internet connection.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdgUEWuAOGel",
        "outputId": "25681c33-1c63-4618-c767-1f896710383e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ No data scraped. Please check the site structure or your internet connection.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = \"https://www.nutrition.gov/usda-ree\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Extract all sections with links and descriptions\n",
        "sections = soup.find_all(\"div\", class_=\"usa-prose\")\n",
        "\n",
        "data = []\n",
        "for section in sections:\n",
        "    for link in section.find_all(\"a\", href=True):\n",
        "        text = link.get_text(strip=True)\n",
        "        href = link['href']\n",
        "        # Only include valid and descriptive links\n",
        "        if text and href.startswith(\"http\"):\n",
        "            data.append({\"Title\": text, \"URL\": href})\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "print(df.head())\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(\"nutrition_usda_links.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82ezcqf9RN8n",
        "outputId": "1b16f6aa-8c95-4d41-dac7-00000289cbd9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ]
    }
  ]
}