{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xBQFOYrWiZN",
        "outputId": "b751445e-c58e-4a1b-f3ae-30e32d2ab279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Scraped and saved recipes to 'themealdb_recipes.csv'\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def get_meals_by_category(category='Beef'):\n",
        "    \"\"\"Get meals list by category from TheMealDB API\"\"\"\n",
        "    url = f\"https://www.themealdb.com/api/json/v1/1/filter.php?c={category}\"\n",
        "    response = requests.get(url)\n",
        "    return response.json().get('meals', [])\n",
        "\n",
        "def get_meal_details(meal_id):\n",
        "    \"\"\"Get full details of a specific meal\"\"\"\n",
        "    url = f\"https://www.themealdb.com/api/json/v1/1/lookup.php?i={meal_id}\"\n",
        "    response = requests.get(url)\n",
        "    meals = response.json().get('meals', [])\n",
        "    return meals[0] if meals else {}\n",
        "\n",
        "def extract_ingredients(meal):\n",
        "    \"\"\"Extract ingredients and measurements as a dictionary\"\"\"\n",
        "    ingredients = {}\n",
        "    for i in range(1, 21):  # TheMealDB supports up to 20 ingredients\n",
        "        ing = meal.get(f\"strIngredient{i}\")\n",
        "        meas = meal.get(f\"strMeasure{i}\")\n",
        "        if ing and ing.strip():\n",
        "            ingredients[ing.strip()] = meas.strip() if meas else ''\n",
        "    return ingredients\n",
        "\n",
        "def scrape_meals(category='Beef', limit=10):\n",
        "    \"\"\"Scrape multiple meals and return as a DataFrame\"\"\"\n",
        "    meals = get_meals_by_category(category)\n",
        "    meal_data = []\n",
        "\n",
        "    for meal in meals[:limit]:\n",
        "        details = get_meal_details(meal['idMeal'])\n",
        "        data = {\n",
        "            'MealID': meal['idMeal'],\n",
        "            'Meal': details.get('strMeal'),\n",
        "            'Category': details.get('strCategory'),\n",
        "            'Area': details.get('strArea'),\n",
        "            'Instructions': details.get('strInstructions'),\n",
        "            'Tags': details.get('strTags'),\n",
        "            'YouTube': details.get('strYoutube'),\n",
        "            'Ingredients': extract_ingredients(details)\n",
        "        }\n",
        "        meal_data.append(data)\n",
        "\n",
        "    return pd.DataFrame(meal_data)\n",
        "\n",
        "# Example usage:\n",
        "df_recipes = scrape_meals(category='Chicken', limit=5)\n",
        "df_recipes.to_csv(\"themealdb_recipes.csv\", index=False)\n",
        "print(\"‚úÖ Scraped and saved recipes to 'themealdb_recipes.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hzj5sq8qml9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def get_all_categories():\n",
        "    url = \"https://www.themealdb.com/api/json/v1/1/list.php?c=list\"\n",
        "    response = requests.get(url)\n",
        "    return [item['strCategory'] for item in response.json().get('meals', [])]\n",
        "\n",
        "def get_meals_by_category(category):\n",
        "    url = f\"https://www.themealdb.com/api/json/v1/1/filter.php?c={category}\"\n",
        "    response = requests.get(url)\n",
        "    return response.json().get('meals', [])\n",
        "\n",
        "def get_meal_details(meal_id):\n",
        "    url = f\"https://www.themealdb.com/api/json/v1/1/lookup.php?i={meal_id}\"\n",
        "    response = requests.get(url)\n",
        "    meals = response.json().get('meals', [])\n",
        "    return meals[0] if meals else {}\n",
        "\n",
        "def extract_ingredients(meal):\n",
        "    ingredients = {}\n",
        "    for i in range(1, 21):\n",
        "        ing = meal.get(f\"strIngredient{i}\")\n",
        "        meas = meal.get(f\"strMeasure{i}\")\n",
        "        if ing and ing.strip():\n",
        "            ingredients[ing.strip()] = meas.strip() if meas else ''\n",
        "    return ingredients\n",
        "\n",
        "def scrape_all_recipes(minimum=100):\n",
        "    all_data = []\n",
        "    seen_ids = set()\n",
        "    categories = get_all_categories()\n",
        "\n",
        "    for category in categories:\n",
        "        meals = get_meals_by_category(category)\n",
        "        for meal in meals:\n",
        "            meal_id = meal['idMeal']\n",
        "            if meal_id not in seen_ids:\n",
        "                details = get_meal_details(meal_id)\n",
        "                recipe = {\n",
        "                    'MealID': meal_id,\n",
        "                    'Meal': details.get('strMeal'),\n",
        "                    'Category': details.get('strCategory'),\n",
        "                    'Area': details.get('strArea'),\n",
        "                    'Instructions': details.get('strInstructions'),\n",
        "                    'Tags': details.get('strTags'),\n",
        "                    'YouTube': details.get('strYoutube'),\n",
        "                    'Ingredients': extract_ingredients(details)\n",
        "                }\n",
        "                all_data.append(recipe)\n",
        "                seen_ids.add(meal_id)\n",
        "                time.sleep(0.2)  # Be polite to the API server\n",
        "\n",
        "                if len(all_data) >= minimum:\n",
        "                    break\n",
        "        if len(all_data) >= minimum:\n",
        "            break\n",
        "\n",
        "    return pd.DataFrame(all_data)\n",
        "\n",
        "# Scrape and save at least 120 recipes\n",
        "df_all_recipes = scrape_all_recipes(minimum=120)\n",
        "df_all_recipes.to_csv(\"all_mealdb_recipes_120+.csv\", index=False)\n",
        "print(f\"‚úÖ Scraped {len(df_all_recipes)} recipes and saved to 'all_mealdb_recipes_120+.csv'\")"
      ],
      "metadata": {
        "id": "EVQZFPZXZd9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def get_all_categories():\n",
        "    url = \"https://www.themealdb.com/api/json/v1/1/list.php?c=list\"\n",
        "    response = requests.get(url)\n",
        "    return [item['strCategory'] for item in response.json().get('meals', [])]\n",
        "\n",
        "def get_meals_by_category(category):\n",
        "    url = f\"https://www.themealdb.com/api/json/v1/1/filter.php?c={category}\"\n",
        "    response = requests.get(url)\n",
        "    return response.json().get('meals', [])\n",
        "\n",
        "def get_meal_details(meal_id):\n",
        "    url = f\"https://www.themealdb.com/api/json/v1/1/lookup.php?i={meal_id}\"\n",
        "    response = requests.get(url)\n",
        "    meals = response.json().get('meals', [])\n",
        "    return meals[0] if meals else {}\n",
        "\n",
        "def extract_ingredients(meal):\n",
        "    ingredients = {}\n",
        "    for i in range(1, 21):\n",
        "        ing = meal.get(f\"strIngredient{i}\")\n",
        "        meas = meal.get(f\"strMeasure{i}\")\n",
        "        if ing and ing.strip():\n",
        "            ingredients[ing.strip()] = meas.strip() if meas else ''\n",
        "    return ingredients\n",
        "\n",
        "def scrape_all_available_recipes():\n",
        "    all_data = []\n",
        "    seen_ids = set()\n",
        "    categories = get_all_categories()\n",
        "\n",
        "    for category in categories:\n",
        "        meals = get_meals_by_category(category)\n",
        "        for meal in meals:\n",
        "            meal_id = meal['idMeal']\n",
        "            if meal_id not in seen_ids:\n",
        "                details = get_meal_details(meal_id)\n",
        "                recipe = {\n",
        "                    'MealID': meal_id,\n",
        "                    'Meal': details.get('strMeal'),\n",
        "                    'Category': details.get('strCategory'),\n",
        "                    'Area': details.get('strArea'),\n",
        "                    'Instructions': details.get('strInstructions'),\n",
        "                    'Tags': details.get('strTags'),\n",
        "                    'YouTube': details.get('strYoutube'),\n",
        "                    'Ingredients': extract_ingredients(details)\n",
        "                }\n",
        "                all_data.append(recipe)\n",
        "                seen_ids.add(meal_id)\n",
        "                time.sleep(0.2)  # Be polite to the API server\n",
        "\n",
        "    return pd.DataFrame(all_data)\n",
        "\n",
        "# Scrape all available recipes (~300+)\n",
        "df_all = scrape_all_available_recipes()\n",
        "df_all.to_csv(\"TheMealDB_all_recipes.csv\", index=False)\n",
        "print(f\"‚úÖ Scraped {len(df_all)} unique recipes and saved to 'TheMealDB_all_recipes.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyrtxmQjZp24",
        "outputId": "3fca265e-4ef6-4787-e47d-1807196c98d0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Scraped 304 unique recipes and saved to 'TheMealDB_all_recipes.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UWVCeP-zsWln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "API_KEY = \"your_usda_api_key_here\"  # Replace this with your actual API key\n",
        "\n",
        "def search_usda_ingredient(query):\n",
        "    \"\"\"Search for an ingredient on USDA FoodData Central\"\"\"\n",
        "    url = f\"https://api.nal.usda.gov/fdc/v1/foods/search\"\n",
        "    params = {\n",
        "        \"query\": query,\n",
        "        \"pageSize\": 1,\n",
        "        \"api_key\": API_KEY\n",
        "    }\n",
        "    r = requests.get(url, params=params)\n",
        "    results = r.json().get(\"foods\", [])\n",
        "    return results[0] if results else None\n",
        "\n",
        "def get_nutrients(fdc_id):\n",
        "    \"\"\"Get nutrient details for a specific food item\"\"\"\n",
        "    url = f\"https://api.nal.usda.gov/fdc/v1/food/{fdc_id}\"\n",
        "    params = {\"api_key\": API_KEY}\n",
        "    r = requests.get(url, params=params)\n",
        "    data = r.json()\n",
        "\n",
        "    nutrients = {}\n",
        "    for item in data.get(\"foodNutrients\", []):\n",
        "        name = item.get(\"nutrientName\", \"\").lower()\n",
        "        amount = item.get(\"value\", 0)\n",
        "        unit = item.get(\"unitName\", \"\")\n",
        "        if name in [\"energy\", \"protein\", \"total lipid (fat)\", \"carbohydrate, by difference\"]:\n",
        "            nutrients[name] = f\"{amount} {unit}\"\n",
        "    return nutrients\n",
        "\n",
        "def estimate_recipe_nutrition(ingredients_dict):\n",
        "    \"\"\"Estimate total nutrition from all ingredients in a recipe\"\"\"\n",
        "    recipe_nutrition = {\n",
        "        'calories_kcal': 0,\n",
        "        'protein_g': 0,\n",
        "        'fat_g': 0,\n",
        "        'carbs_g': 0\n",
        "    }\n",
        "\n",
        "    for ingredient in ingredients_dict:\n",
        "        try:\n",
        "            item = search_usda_ingredient(ingredient)\n",
        "            if not item: continue\n",
        "\n",
        "            nutrients = get_nutrients(item[\"fdcId\"])\n",
        "            # Convert to standard grams for rough estimates (100g servings)\n",
        "            recipe_nutrition['calories_kcal'] += float(nutrients.get(\"energy\", \"0 kcal\").split()[0])\n",
        "            recipe_nutrition['protein_g'] += float(nutrients.get(\"protein\", \"0 g\").split()[0])\n",
        "            recipe_nutrition['fat_g'] += float(nutrients.get(\"total lipid (fat)\", \"0 g\").split()[0])\n",
        "            recipe_nutrition['carbs_g'] += float(nutrients.get(\"carbohydrate, by difference\", \"0 g\").split()[0])\n",
        "\n",
        "            time.sleep(0.3)  # Avoid rate limits\n",
        "        except Exception as e:\n",
        "            print(f\"Error with ingredient '{ingredient}': {e}\")\n",
        "\n",
        "    return recipe_nutrition\n",
        "\n",
        "# Load your previously scraped CSV\n",
        "df = pd.read_csv(\"TheMealDB_all_recipes.csv\")\n",
        "df['EstimatedCalories_kcal'] = 0.0\n",
        "df['EstimatedProtein_g'] = 0.0\n",
        "df['EstimatedFat_g'] = 0.0\n",
        "df['EstimatedCarbs_g'] = 0.0\n",
        "\n",
        "# Ingredients column is a stringified dict, so convert it\n",
        "import ast\n",
        "df['Ingredients'] = df['Ingredients'].apply(ast.literal_eval)\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    nutrition = estimate_recipe_nutrition(row['Ingredients'])\n",
        "    df.at[idx, 'EstimatedCalories_kcal'] = nutrition['calories_kcal']\n",
        "    df.at[idx, 'EstimatedProtein_g'] = nutrition['protein_g']\n",
        "    df.at[idx, 'EstimatedFat_g'] = nutrition['fat_g']\n",
        "    df.at[idx, 'EstimatedCarbs_g'] = nutrition['carbs_g']\n",
        "    print(f\"Processed: {row['Meal']}\")\n",
        "\n",
        "# Save enriched dataset\n",
        "df.to_csv(\"TheMealDB_with_nutrition.csv\", index=False)\n",
        "print(\"‚úÖ Nutrition values added and saved to 'TheMealDB_with_nutrition.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxfxGyKTZ5Wx",
        "outputId": "7d27050e-352f-4229-c49f-c7c6f35132aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed: Beef and Mustard Pie\n",
            "Processed: Beef and Oyster pie\n",
            "Processed: Beef Asado\n",
            "Processed: Beef Banh Mi Bowls with Sriracha Mayo, Carrot & Pickled Cucumber\n",
            "Processed: Beef Bourguignon\n",
            "Processed: Beef Brisket Pot Roast\n",
            "Processed: Beef Caldereta\n",
            "Processed: Beef Dumpling Stew\n",
            "Processed: Beef Lo Mein\n",
            "Processed: Beef Mechado\n",
            "Processed: Beef Rendang\n",
            "Processed: Beef stroganoff\n",
            "Processed: Beef Sunday Roast\n",
            "Processed: Beef Wellington\n",
            "Processed: Big Mac\n",
            "Processed: Bistek\n",
            "Processed: Bitterballen (Dutch meatballs)\n",
            "Processed: Braised Beef Chilli\n",
            "Processed: Cevapi Sausages\n",
            "Processed: Chivito uruguayo\n",
            "Processed: Corned Beef and Cabbage\n",
            "Processed: Croatian Bean Stew\n",
            "Processed: Croatian lamb peka\n",
            "Processed: Egyptian Fatteh\n",
            "Processed: Golabki (cabbage roll)\n",
            "Processed: Irish stew\n",
            "Processed: Jamaican Beef Patties\n",
            "Processed: Ma Po Tofu\n",
            "Processed: Massaman Beef curry\n",
            "Processed: Minced Beef Pie\n",
            "Processed: Montreal Smoked Meat\n",
            "Processed: Moussaka\n",
            "Processed: Mulukhiyah\n",
            "Processed: Oxtail with broad beans\n",
            "Processed: Paszteciki (Polish Pasties)\n",
            "Processed: Pate Chinois\n",
            "Processed: Portuguese prego with green piri-piri\n",
            "Processed: Red Peas Soup\n",
            "Processed: Roti john\n",
            "Processed: Soy-Glazed Meatloaves with Wasabi Mashed Potatoes & Roasted Carrots\n",
            "Processed: Spaghetti Bolognese\n",
            "Processed: Steak and Kidney Pie\n",
            "Processed: Steak Diane\n",
            "Processed: Szechuan Beef\n",
            "Processed: Traditional Croatian Goulash\n",
            "Processed: Vegetable Shepherds Pie\n",
            "Processed: Bread omelette\n",
            "Processed: Breakfast Potatoes\n",
            "Processed: English Breakfast\n",
            "Processed: Fruit and Cream Cheese Breakfast Pastries\n",
            "Processed: Full English Breakfast\n",
            "Processed: Home-made Mandazi\n",
            "Processed: Salmon Eggs Eggs Benedict\n",
            "Processed: Smoked Haddock Kedgeree\n",
            "Processed: 15-minute chicken & halloumi burgers\n",
            "Processed: Ayam Percik\n",
            "Processed: Brown Stew Chicken\n",
            "Processed: Chick-Fil-A Sandwich\n",
            "Processed: Chicken & mushroom Hotpot\n",
            "Processed: Chicken Alfredo Primavera\n",
            "Processed: Chicken Basquaise\n",
            "Processed: Chicken Congee\n",
            "Processed: Chicken Couscous\n",
            "Processed: Chicken Enchilada Casserole\n",
            "Processed: Chicken Fajita Mac and Cheese\n",
            "Processed: Chicken Ham and Leek Pie\n",
            "Processed: Chicken Handi\n",
            "Processed: Chicken Karaage\n",
            "Processed: Chicken Marengo\n",
            "Processed: Chicken Parmentier\n",
            "Processed: Chicken Quinoa Greek Salad\n",
            "Processed: Coq au vin\n",
            "Processed: Crock Pot Chicken Baked Tacos\n",
            "Processed: French Onion Chicken with Roasted Carrots & Mashed Potatoes\n",
            "Processed: General Tsos Chicken\n",
            "Processed: Honey Balsamic Chicken with Crispy Broccoli & Potatoes\n",
            "Processed: Jerk chicken with rice & peas\n",
            "Processed: Katsu Chicken curry\n",
            "Processed: Kentucky Fried Chicken\n",
            "Processed: Kung Pao Chicken\n",
            "Processed: Nutty Chicken Curry\n",
            "Processed: Pad See Ew\n",
            "Processed: Piri-piri chicken and slaw\n",
            "Processed: Potato Gratin with Chicken\n",
            "Processed: Rappie Pie\n",
            "Processed: Rosol (Polish Chicken Soup)\n",
            "Processed: Shawarma\n",
            "Processed: Tandoori chicken\n",
            "Processed: Teriyaki Chicken Casserole\n",
            "Processed: Thai Green Curry\n",
            "Processed: Apam balik\n",
            "Processed: Apple & Blackberry Crumble\n",
            "Processed: Apple Frangipan Tart\n",
            "Processed: Bakewell tart\n",
            "Processed: Banana Pancakes\n",
            "Processed: Battenberg Cake\n",
            "Processed: BeaverTails\n",
            "Processed: Blackberry Fool\n",
            "Processed: Bread and Butter Pudding\n",
            "Processed: Budino Di Ricotta\n",
            "Processed: Canadian Butter Tarts\n",
            "Processed: Carrot Cake\n",
            "Processed: Cashew Ghoriba Biscuits\n",
            "Processed: Chelsea Buns\n",
            "Processed: Chinon Apple Tarts\n",
            "Processed: Choc Chip Pecan Pie\n",
            "Processed: Chocolate Avocado Mousse\n",
            "Processed: Chocolate Caramel Crispy\n",
            "Processed: Chocolate Gateau\n",
            "Processed: Chocolate Raspberry Brownies\n",
            "Processed: Chocolate Souffle\n",
            "Processed: Christmas cake\n",
            "Processed: Christmas Pudding Flapjack\n",
            "Processed: Christmas Pudding Trifle\n",
            "Processed: Classic Christmas pudding\n",
            "Processed: Dundee cake\n",
            "Processed: Eccles Cakes\n",
            "Processed: Eton Mess\n",
            "Processed: Honey Yogurt Cheesecake\n",
            "Processed: Hot Chocolate Fudge\n",
            "Processed: Jam Roly-Poly\n",
            "Processed: Key Lime Pie\n",
            "Processed: Krispy Kreme Donut\n",
            "Processed: Madeira Cake\n",
            "Processed: Mince Pies\n",
            "Processed: Nanaimo Bars\n",
            "Processed: New York cheesecake\n",
            "Processed: Pancakes\n",
            "Processed: Parkin Cake\n",
            "Processed: Peach & Blueberry Grunt\n",
            "Processed: Peanut Butter Cheesecake\n",
            "Processed: Peanut Butter Cookies\n",
            "Processed: Pear Tarte Tatin\n",
            "Processed: Polskie Nalesniki (Polish Pancakes)\n",
            "Processed: Portuguese custard tarts\n",
            "Processed: Pouding chomeur\n",
            "Processed: Pumpkin Pie\n",
            "Processed: Rock Cakes\n",
            "Processed: Rocky Road Fudge\n",
            "Processed: Rogaliki (Polish Croissant Cookies)\n",
            "Processed: Salted Caramel Cheescake\n",
            "Processed: Seri muka kuih\n",
            "Processed: Spotted Dick\n",
            "Processed: Sticky Toffee Pudding\n",
            "Processed: Sticky Toffee Pudding Ultimate\n",
            "Processed: Strawberries Romanoff\n",
            "Processed: Strawberry Rhubarb Pie\n",
            "Processed: Sugar Pie\n",
            "Processed: Summer Pudding\n",
            "Processed: Tarte Tatin\n",
            "Processed: Timbits\n",
            "Processed: Treacle Tart\n",
            "Processed: Tunisian Orange Cake\n",
            "Processed: Walnut Roll Gu≈ævara\n",
            "Processed: White chocolate creme brulee\n",
            "Processed: Mbuzi Choma (Roasted Goat)\n",
            "Processed: Kapsalon\n",
            "Processed: Keleya Zaara\n",
            "Processed: Lamb and Lemon Souvlaki\n",
            "Processed: Lamb and Potato pie\n",
            "Processed: Lamb Biryani\n",
            "Processed: Lamb Pilaf (Plov)\n",
            "Processed: Lamb Rogan josh\n",
            "Processed: Lamb Tagine\n",
            "Processed: Lamb tomato and sweet spices\n",
            "Processed: Lamb Tzatziki Burgers\n",
            "Processed: Lancashire hotpot\n",
            "Processed: McSinghs Scotch pie\n",
            "Processed: Rigatoni with fennel sausage sauce\n",
            "Processed: Stuffed Lamb Tomatoes\n",
            "Processed: Tunisian Lamb Soup\n",
            "Processed: Bean & Sausage Hotpot\n",
            "Processed: Callaloo Jamaican Style\n",
            "Processed: Chakchouka \n",
            "Processed: Duck Confit\n",
            "Processed: French Lentils With Garlic and Thyme\n",
            "Processed: French Omelette\n",
            "Processed: Migas\n",
            "Processed: Osso Buco alla Milanese\n",
            "Processed: Pizza Express Margherita\n",
            "Processed: Poutine\n",
            "Processed: Three-cheese souffles\n",
            "Processed: Turkey Meatloaf\n",
            "Processed: Chilli prawn linguine\n",
            "Processed: Fettuccine Alfredo\n",
            "Processed: Fettucine alfredo\n",
            "Processed: Grilled Mac and Cheese Sandwich\n",
            "Processed: Lasagna Sandwiches\n",
            "Processed: Lasagne\n",
            "Processed: Pilchard puttanesca\n",
            "Processed: Spaghetti alla Carbonara\n",
            "Processed: Venetian Duck Ragu\n",
            "Processed:  Bubble & Squeak\n",
            "Processed: BBQ Pork Sloppy Joes\n",
            "Processed: Bigos (Hunters Stew)\n",
            "Processed: Boxty Breakfast\n",
            "Processed: Coddled pork with cider\n",
            "Processed: Crispy Sausages and Greens\n",
            "Processed: Ham hock colcannon\n",
            "Processed: Hot and Sour Soup\n",
            "Processed: Japanese Katsudon\n",
            "Processed: Pork Cassoulet\n",
            "Processed: Portuguese barbecued pork (Febras assadas)\n",
            "Processed: Skillet Apple Pork Chops with Roasted Sweet Potatoes & Zucchini\n",
            "Processed: Stamppot\n",
            "Processed: Sweet and Sour Pork\n",
            "Processed: Toad In The Hole\n",
            "Processed: Tonkatsu pork\n",
            "Processed: Tourtiere\n",
            "Processed: Vietnamese Grilled Pork (bun-thit-nuong)\n",
            "Processed: Wontons\n",
            "Processed: Baked salmon with fennel & tomatoes\n",
            "Processed: Cajun spiced fish tacos\n",
            "Processed: Escovitch Fish\n",
            "Processed: Fish fofos\n",
            "Processed: Fish pie\n",
            "Processed: Fish Soup (Ukha)\n",
            "Processed: Fish Stew with Rouille\n",
            "Processed: Garides Saganaki\n",
            "Processed: Grilled Portuguese sardines\n",
            "Processed: Honey Teriyaki Salmon\n",
            "Processed: Kedgeree\n",
            "Processed: Kung Po Prawns\n",
            "Processed: Laksa King Prawn Noodles\n",
            "Processed: Mediterranean Pasta Salad\n",
            "Processed: Mee goreng mamak\n",
            "Processed: Nasi lemak\n",
            "Processed: Portuguese fish stew (Caldeirada de peixe)\n",
            "Processed: Recheado Masala Fish\n",
            "Processed: Salmon Avocado Salad\n",
            "Processed: Salmon Prawn Risotto\n",
            "Processed: Saltfish and Ackee\n",
            "Processed: Seafood fideu√†\n",
            "Processed: Shrimp Chow Fun\n",
            "Processed: Sledz w Oleju (Polish Herrings)\n",
            "Processed: Spring onion and prawn empanadas\n",
            "Processed: Sushi\n",
            "Processed: Three Fish Pie\n",
            "Processed: Tuna and Egg Briks\n",
            "Processed: Tuna Nicoise\n",
            "Processed: Blini Pancakes\n",
            "Processed: Boulang√®re Potatoes\n",
            "Processed: Brie wrapped in prosciutto & brioche\n",
            "Processed: Burek\n",
            "Processed: Corba\n",
            "Processed: Fennel Dauphinoise\n",
            "Processed: Feteer Meshaltet\n",
            "Processed: French Onion Soup\n",
            "Processed: Fresh sardines\n",
            "Processed: Japanese gohan rice\n",
            "Processed: Kumpir\n",
            "Processed: Mushroom soup with buckwheat\n",
            "Processed: Mustard champ\n",
            "Processed: Pierogi (Polish Dumplings)\n",
            "Processed: Prawn & Fennel Bisque\n",
            "Processed: Snert (Dutch Split Pea Soup)\n",
            "Processed: Split Pea Soup\n",
            "Processed: Broccoli & Stilton soup\n",
            "Processed: Clam chowder\n",
            "Processed: Cream Cheese Tart\n",
            "Processed: Creamy Tomato Soup\n",
            "Processed: Roast fennel and aubergine paella\n",
            "Processed: Vegan Chocolate Cake\n",
            "Processed: Vegan Lasagna\n",
            "Processed: Baingan Bharta\n",
            "Processed: Beetroot Soup (Borscht)\n",
            "Processed: Cabbage Soup (Shchi)\n",
            "Processed: Chickpea Fajitas\n",
            "Processed: Crispy Eggplant\n",
            "Processed: Dal fry\n",
            "Processed: Egg Drop Soup\n",
            "Processed: Eggplant Adobo\n",
            "Processed: Flamiche\n",
            "Processed: Ful Medames\n",
            "Processed: Gigantes Plaki\n",
            "Processed: Grilled eggplant with coconut milk\n",
            "Processed: Kafteji\n",
            "Processed: Kidney Bean Curry\n",
            "Processed: Koshari\n",
            "Processed: Leblebi Soup\n",
            "Processed: Matar Paneer\n",
            "Processed: Moroccan Carrot Soup\n",
            "Processed: Mushroom & Chestnut Rotolo\n",
            "Processed: Potato Salad (Olivier Salad)\n",
            "Processed: Proven√ßal Omelette Cake\n",
            "Processed: Ratatouille\n",
            "Processed: Ribollita\n",
            "Processed: Roasted Eggplant With Tahini, Pine Nuts, and Lentils\n",
            "Processed: Shakshuka\n",
            "Processed: Smoky Lentil Chili with Squash\n",
            "Processed: Spanish Tortilla\n",
            "Processed: Spicy Arrabiata Penne\n",
            "Processed: Spicy North African Potato Salad\n",
            "Processed: Spinach & Ricotta Cannelloni\n",
            "Processed: Squash linguine\n",
            "Processed: Stovetop Eggplant With Harissa, Chickpeas, and Cumin Yogurt\n",
            "Processed: Stuffed Bell Peppers with Quinoa and Black Beans\n",
            "Processed: Summer Pistou\n",
            "Processed: Tahini Lentils\n",
            "Processed: Tamiya\n",
            "Processed: Tortang Talong\n",
            "Processed: Vegetarian Casserole\n",
            "Processed: Vegetarian Chilli\n",
            "Processed: Yaki Udon\n",
            "‚úÖ Nutrition values added and saved to 'TheMealDB_with_nutrition.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Replace with your own credentials\n",
        "APP_ID = 'your_app_id'\n",
        "APP_KEY = 'your_app_key'\n",
        "\n",
        "def get_recipes(query, max_results=50):\n",
        "    url = 'https://api.edamam.com/api/recipes/v2'\n",
        "    params = {\n",
        "        'type': 'public',\n",
        "        'q': query,\n",
        "        'app_id': APP_ID,\n",
        "        'app_key': APP_KEY,\n",
        "        'random': 'true'\n",
        "    }\n",
        "    recipes = []\n",
        "    seen_uris = set()\n",
        "\n",
        "    while len(recipes) < max_results:\n",
        "        response = requests.get(url, params=params)\n",
        "        data = response.json()\n",
        "        hits = data.get('hits', [])\n",
        "        for hit in hits:\n",
        "            recipe_data = hit['recipe']\n",
        "            uri = recipe_data['uri']\n",
        "            if uri in seen_uris:\n",
        "                continue\n",
        "            seen_uris.add(uri)\n",
        "\n",
        "            recipe = {\n",
        "                'Label': recipe_data['label'],\n",
        "                'Source': recipe_data['source'],\n",
        "                'URL': recipe_data['url'],\n",
        "                'Servings': recipe_data['yield'],\n",
        "                'Calories': recipe_data['calories'],\n",
        "                'TotalWeight': recipe_data['totalWeight'],\n",
        "                'CuisineType': recipe_data.get('cuisineType', []),\n",
        "                'MealType': recipe_data.get('mealType', []),\n",
        "                'DishType': recipe_data.get('dishType', []),\n",
        "                'Ingredients': '; '.join([i['text'] for i in recipe_data['ingredients']]),\n",
        "                'Nutrients': recipe_data['totalNutrients']\n",
        "            }\n",
        "            recipes.append(recipe)\n",
        "\n",
        "            if len(recipes) >= max_results:\n",
        "                break\n",
        "\n",
        "        if '_links' in data and 'next' in data['_links']:\n",
        "            url = data['_links']['next']['href']\n",
        "            params = None  # URL already contains all parameters\n",
        "            time.sleep(1)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    return recipes\n",
        "\n",
        "def flatten_nutrients(nutrients_dict):\n",
        "    flattened = {}\n",
        "    for k, v in nutrients_dict.items():\n",
        "        flattened[f\"{k}_{v.get('label')}\"] = v.get('quantity')\n",
        "    return flattened\n",
        "\n",
        "def build_dataframe(recipes):\n",
        "    records = []\n",
        "    for r in recipes:\n",
        "        base = {\n",
        "            'Label': r['Label'],\n",
        "            'Source': r['Source'],\n",
        "            'URL': r['URL'],\n",
        "            'Servings': r['Servings'],\n",
        "            'Calories': r['Calories'],\n",
        "            'TotalWeight': r['TotalWeight'],\n",
        "            'CuisineType': ', '.join(r['CuisineType']),\n",
        "            'MealType': ', '.join(r['MealType']),\n",
        "            'DishType': ', '.join(r['DishType']),\n",
        "            'Ingredients': r['Ingredients']\n",
        "        }\n",
        "        nutrients = flatten_nutrients(r['Nutrients'])\n",
        "        base.update(nutrients)\n",
        "        records.append(base)\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "# Run the scraper\n",
        "recipes = get_recipes(\"chicken\", max_results=50)\n",
        "df = build_dataframe(recipes)\n",
        "df.to_csv(\"Edamam_chicken_recipes_with_nutrients.csv\", index=False)\n",
        "print(f\"‚úÖ Saved {len(df)} recipes with nutrient info to 'Edamam_chicken_recipes_with_nutrients.csv'\")\n"
      ],
      "metadata": {
        "id": "Z55ve0B4sYbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load both recipe files\n",
        "df_120 = pd.read_csv(\"themealdb_recipes.csv\")\n",
        "df_302 = pd.read_csv(\"TheMealDB_all_recipes.csv\")\n",
        "\n",
        "# Concatenate both datasets\n",
        "df_combined = pd.concat([df_120, df_302], ignore_index=True)\n",
        "\n",
        "# Drop duplicates based on MealID\n",
        "df_combined = df_combined.drop_duplicates(subset='MealID')\n",
        "\n",
        "# Save the merged dataset\n",
        "df_combined.to_csv(\"TheMealDB_merged_recipes.csv\", index=False)\n",
        "\n",
        "print(f\"‚úÖ Merged dataset saved with {len(df_combined)} unique recipes.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5KXGM5emoxq",
        "outputId": "1c49a15d-21c3-4687-c8e7-c9c5dc272dae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Merged dataset saved with 304 unique recipes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "API_KEY = \"your_usda_api_key\"\n",
        "\n",
        "def search_foods(query, max_results=20):\n",
        "    url = \"https://api.nal.usda.gov/fdc/v1/foods/search\"\n",
        "    params = {\n",
        "        \"api_key\": API_KEY,\n",
        "        \"query\": query,\n",
        "        \"pageSize\": max_results\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    return response.json().get(\"foods\", [])\n",
        "\n",
        "def get_food_details(fdc_id):\n",
        "    url = f\"https://api.nal.usda.gov/fdc/v1/food/{fdc_id}\"\n",
        "    params = {\"api_key\": API_KEY}\n",
        "    response = requests.get(url, params=params)\n",
        "    return response.json()\n",
        "\n",
        "def build_food_nutrient_dataset(query, max_results=20):\n",
        "    results = []\n",
        "    foods = search_foods(query, max_results=max_results)\n",
        "    for food in foods:\n",
        "        fdc_id = food[\"fdcId\"]\n",
        "        details = get_food_details(fdc_id)\n",
        "        nutrients = {n[\"nutrientName\"]: n[\"value\"] for n in details.get(\"foodNutrients\", [])}\n",
        "        record = {\n",
        "            \"FDC_ID\": fdc_id,\n",
        "            \"Description\": details.get(\"description\"),\n",
        "            \"Brand\": details.get(\"brandOwner\", \"\"),\n",
        "            **nutrients\n",
        "        }\n",
        "        results.append(record)\n",
        "        time.sleep(0.2)\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Example: get nutrient data for 20 common foods related to \"chicken\"\n",
        "df = build_food_nutrient_dataset(\"chicken\", max_results=20)\n",
        "df.to_csv(\"USDA_chicken_ingredients_nutrients.csv\", index=False)\n",
        "print(f\"‚úÖ Saved {len(df)} items with nutrient values to 'USDA_chicken_ingredients_nutrients.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlhiPuSItKk4",
        "outputId": "efdf905c-1f5f-43e7-c830-2abbb9df6a2f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved 0 items with nutrient values to 'USDA_chicken_ingredients_nutrients.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def get_tasty_recipe_links(pages=10):\n",
        "    base_url = \"https://tasty.co/page/\"\n",
        "    recipe_links = []\n",
        "\n",
        "    for page in range(1, pages + 1):\n",
        "        url = base_url + str(page)\n",
        "        res = requests.get(url)\n",
        "        soup = BeautifulSoup(res.content, \"html.parser\")\n",
        "        for link in soup.find_all(\"a\", href=True):\n",
        "            href = link[\"href\"]\n",
        "            if href.startswith(\"/recipe/\") and href not in recipe_links:\n",
        "                recipe_links.append(\"https://tasty.co\" + href)\n",
        "        time.sleep(1)\n",
        "    return list(set(recipe_links))\n",
        "\n",
        "def scrape_tasty_recipe(url):\n",
        "    res = requests.get(url)\n",
        "    soup = BeautifulSoup(res.content, \"html.parser\")\n",
        "\n",
        "    title = soup.find(\"h1\").text.strip() if soup.find(\"h1\") else \"\"\n",
        "    ingredients = [li.text.strip() for li in soup.select(\".ingredients__section li\")]\n",
        "    instructions = [step.text.strip() for step in soup.select(\".prep-steps li\")]\n",
        "    nutrition = soup.find(\"div\", class_=\"nutrition\")\n",
        "\n",
        "    return {\n",
        "        \"Title\": title,\n",
        "        \"Ingredients\": \"; \".join(ingredients),\n",
        "        \"Instructions\": \" | \".join(instructions),\n",
        "        \"Nutrition\": nutrition.text.strip() if nutrition else \"N/A\",\n",
        "        \"URL\": url\n",
        "    }\n",
        "\n",
        "def scrape_tasty_recipes(max_recipes=300):\n",
        "    links = get_tasty_recipe_links(pages=30)\n",
        "    print(f\"üîó Found {len(links)} recipe links.\")\n",
        "    all_recipes = []\n",
        "\n",
        "    for i, link in enumerate(links[:max_recipes]):\n",
        "        print(f\"Scraping ({i+1}/{max_recipes}): {link}\")\n",
        "        try:\n",
        "            recipe = scrape_tasty_recipe(link)\n",
        "            all_recipes.append(recipe)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to scrape: {e}\")\n",
        "        time.sleep(1)\n",
        "\n",
        "    return pd.DataFrame(all_recipes)\n",
        "\n",
        "# Run it\n",
        "df = scrape_tasty_recipes(max_recipes=300)\n",
        "df.to_csv(\"tasty_300_recipes_with_nutrition.csv\", index=False)\n",
        "print(f\"‚úÖ Saved {len(df)} recipes to 'tasty_300_recipes_with_nutrition.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S7aNh2otvzy",
        "outputId": "8e182019-8f1d-46e6-d2a5-7e98539ba150"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîó Found 0 recipe links.\n",
            "‚úÖ Saved 0 recipes to 'tasty_300_recipes_with_nutrition.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install recipe-scrapers pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waHnJF7bukQ7",
        "outputId": "f4a892b6-c064-4fa7-e7f5-35bb7a8aad38"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting recipe-scrapers\n",
            "  Downloading recipe_scrapers-15.7.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.12.3 in /usr/local/lib/python3.11/dist-packages (from recipe-scrapers) (4.13.4)\n",
            "Collecting extruct>=0.17.0 (from recipe-scrapers)\n",
            "  Downloading extruct-0.18.0-py2.py3-none-any.whl.metadata (36 kB)\n",
            "Collecting isodate>=0.6.1 (from recipe-scrapers)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.12.3->recipe-scrapers) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.12.3->recipe-scrapers) (4.13.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from extruct>=0.17.0->recipe-scrapers) (5.4.0)\n",
            "Collecting lxml-html-clean (from extruct>=0.17.0->recipe-scrapers)\n",
            "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting rdflib>=6.0.0 (from extruct>=0.17.0->recipe-scrapers)\n",
            "  Downloading rdflib-7.1.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting pyrdfa3 (from extruct>=0.17.0->recipe-scrapers)\n",
            "  Downloading pyRdfa3-3.6.4-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting mf2py (from extruct>=0.17.0->recipe-scrapers)\n",
            "  Downloading mf2py-2.0.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting w3lib (from extruct>=0.17.0->recipe-scrapers)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting html-text>=0.5.1 (from extruct>=0.17.0->recipe-scrapers)\n",
            "  Downloading html_text-0.7.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting jstyleson (from extruct>=0.17.0->recipe-scrapers)\n",
            "  Downloading jstyleson-0.0.2.tar.gz (2.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib>=6.0.0->extruct>=0.17.0->recipe-scrapers) (3.2.3)\n",
            "Requirement already satisfied: html5lib<2.0,>=1.1 in /usr/local/lib/python3.11/dist-packages (from mf2py->extruct>=0.17.0->recipe-scrapers) (1.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.2 in /usr/local/lib/python3.11/dist-packages (from mf2py->extruct>=0.17.0->recipe-scrapers) (2.32.3)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib<2.0,>=1.1->mf2py->extruct>=0.17.0->recipe-scrapers) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.2->mf2py->extruct>=0.17.0->recipe-scrapers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.2->mf2py->extruct>=0.17.0->recipe-scrapers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.2->mf2py->extruct>=0.17.0->recipe-scrapers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.28.2->mf2py->extruct>=0.17.0->recipe-scrapers) (2025.4.26)\n",
            "Downloading recipe_scrapers-15.7.1-py3-none-any.whl (254 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m254.2/254.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading extruct-0.18.0-py2.py3-none-any.whl (26 kB)\n",
            "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading html_text-0.7.0-py3-none-any.whl (8.1 kB)\n",
            "Downloading rdflib-7.1.4-py3-none-any.whl (565 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m565.1/565.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading mf2py-2.0.1-py3-none-any.whl (25 kB)\n",
            "Downloading pyRdfa3-3.6.4-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Building wheels for collected packages: jstyleson\n",
            "  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jstyleson: filename=jstyleson-0.0.2-py3-none-any.whl size=2383 sha256=4e4b7fd8cb6920f631e67928b7e0af0c7217963e1f70197c1128c77e0d71e09f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/63/0e/50090147fb424100f7d9078b71c21b9e7468b6f643515a60d6\n",
            "Successfully built jstyleson\n",
            "Installing collected packages: jstyleson, w3lib, rdflib, lxml-html-clean, isodate, pyrdfa3, mf2py, html-text, extruct, recipe-scrapers\n",
            "Successfully installed extruct-0.18.0 html-text-0.7.0 isodate-0.7.2 jstyleson-0.0.2 lxml-html-clean-0.4.2 mf2py-2.0.1 pyrdfa3-3.6.4 rdflib-7.1.4 recipe-scrapers-15.7.1 w3lib-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "from recipe_scrapers import scrape_me\n",
        "\n",
        "def get_recipe_links(category_url, max_recipes=300):\n",
        "    recipe_links = []\n",
        "    page = 1\n",
        "    while len(recipe_links) < max_recipes:\n",
        "        url = f\"{category_url}?page={page}\"\n",
        "        print(f\"Fetching: {url}\")\n",
        "        response = requests.get(url)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch page {page}\")\n",
        "            break\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        links = soup.select('a.card__titleLink')\n",
        "        if not links:\n",
        "            print(\"No more recipes found.\")\n",
        "            break\n",
        "        for link in links:\n",
        "            href = link.get('href')\n",
        "            if href and href.startswith('https://www.allrecipes.com/recipe/'):\n",
        "                recipe_links.append(href)\n",
        "                if len(recipe_links) >= max_recipes:\n",
        "                    break\n",
        "        page += 1\n",
        "        time.sleep(1)  # Be polite to the server\n",
        "    return recipe_links\n",
        "\n",
        "def scrape_recipe(url):\n",
        "    try:\n",
        "        scraper = scrape_me(url)\n",
        "        return {\n",
        "            'Title': scraper.title(),\n",
        "            'Total Time': scraper.total_time(),\n",
        "            'Yields': scraper.yields(),\n",
        "            'Ingredients': scraper.ingredients(),\n",
        "            'Instructions': scraper.instructions(),\n",
        "            'Nutrients': scraper.nutrients() if hasattr(scraper, 'nutrients') else None,\n",
        "            'URL': url\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    category_url = 'https://www.allrecipes.com/recipes/78/breakfast-and-brunch/'\n",
        "    recipe_urls = get_recipe_links(category_url, max_recipes=300)\n",
        "    print(f\"Found {len(recipe_urls)} recipe URLs.\")\n",
        "\n",
        "    data = []\n",
        "    for idx, url in enumerate(recipe_urls, 1):\n",
        "        print(f\"Scraping ({idx}/{len(recipe_urls)}): {url}\")\n",
        "        recipe = scrape_recipe(url)\n",
        "        if recipe:\n",
        "            data.append(recipe)\n",
        "        time.sleep(1)  # Be polite to the server\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv('allrecipes_recipes.csv', index=False)\n",
        "    print(f\"‚úÖ Saved {len(df)} recipes to 'allrecipes_recipes.csv'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKoHR_mDur9s",
        "outputId": "66abc6fe-bc2d-4965-ebfc-b30889c3c41d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching: https://www.allrecipes.com/recipes/78/breakfast-and-brunch/?page=1\n",
            "Failed to fetch page 1\n",
            "Found 0 recipe URLs.\n",
            "‚úÖ Saved 0 recipes to 'allrecipes_recipes.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required packages\n",
        "!pip install requests beautifulsoup4 pandas\n",
        "\n",
        "# STEP 2: Import necessary libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "\n",
        "# STEP 3: Get recipe URLs from Tasty browse pages\n",
        "def get_recipe_urls(pages=30):\n",
        "    recipe_urls = set()\n",
        "    for page in range(1, pages + 1):\n",
        "        print(f\"üîé Fetching page {page}...\")\n",
        "        try:\n",
        "            url = f\"https://tasty.co/browse?page={page}\"\n",
        "            response = requests.get(url)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            for a in soup.find_all('a', href=True):\n",
        "                href = a['href']\n",
        "                if href.startswith(\"/recipe/\"):\n",
        "                    full_url = \"https://tasty.co\" + href.split(\"?\")[0]\n",
        "                    recipe_urls.add(full_url)\n",
        "            time.sleep(1)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed on page {page}: {e}\")\n",
        "    return list(recipe_urls)\n",
        "\n",
        "# STEP 4: Extract recipe data from JSON-LD\n",
        "def extract_recipe_data(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        scripts = soup.find_all('script', type='application/ld+json')\n",
        "        for script in scripts:\n",
        "            try:\n",
        "                data = json.loads(script.string)\n",
        "                if isinstance(data, list):\n",
        "                    for entry in data:\n",
        "                        if entry.get('@type') == 'Recipe':\n",
        "                            return entry\n",
        "                elif data.get('@type') == 'Recipe':\n",
        "                    return data\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error scraping {url}: {e}\")\n",
        "    return None\n",
        "\n",
        "# STEP 5: Scrape all data\n",
        "recipe_urls = get_recipe_urls(pages=30)  # You can increase pages for more\n",
        "print(f\"‚úÖ Found {len(recipe_urls)} unique recipe URLs.\")\n",
        "\n",
        "recipes = []\n",
        "for idx, url in enumerate(recipe_urls):\n",
        "    print(f\"üì¶ Scraping {idx+1}/{len(recipe_urls)}: {url}\")\n",
        "    data = extract_recipe_data(url)\n",
        "    if data:\n",
        "        recipes.append({\n",
        "            \"Title\": data.get(\"name\", \"\"),\n",
        "            \"Ingredients\": \"\\n\".join(data.get(\"recipeIngredient\", [])),\n",
        "            \"Instructions\": data.get(\"recipeInstructions\", \"\"),\n",
        "            \"Nutrition\": data.get(\"nutrition\", {}),\n",
        "            \"URL\": url\n",
        "        })\n",
        "    time.sleep(1)  # Be polite\n",
        "\n",
        "# STEP 6: Save to CSV\n",
        "df = pd.DataFrame(recipes)\n",
        "df.to_csv(\"tasty_recipes.csv\", index=False)\n",
        "print(f\"\\n‚úÖ Scraped {len(df)} recipes and saved to 'tasty_recipes.csv'\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        },
        "id": "-CGsZ-aDzAy9",
        "outputId": "b21fa634-40ad-4e26-b71c-6bea44e0d95b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "üîé Fetching page 1...\n",
            "üîé Fetching page 2...\n",
            "üîé Fetching page 3...\n",
            "üîé Fetching page 4...\n",
            "üîé Fetching page 5...\n",
            "üîé Fetching page 6...\n",
            "üîé Fetching page 7...\n",
            "üîé Fetching page 8...\n",
            "üîé Fetching page 9...\n",
            "üîé Fetching page 10...\n",
            "üîé Fetching page 11...\n",
            "üîé Fetching page 12...\n",
            "üîé Fetching page 13...\n",
            "üîé Fetching page 14...\n",
            "üîé Fetching page 15...\n",
            "üîé Fetching page 16...\n",
            "üîé Fetching page 17...\n",
            "üîé Fetching page 18...\n",
            "üîé Fetching page 19...\n",
            "üîé Fetching page 20...\n",
            "üîé Fetching page 21...\n",
            "üîé Fetching page 22...\n",
            "üîé Fetching page 23...\n",
            "üîé Fetching page 24...\n",
            "üîé Fetching page 25...\n",
            "üîé Fetching page 26...\n",
            "üîé Fetching page 27...\n",
            "üîé Fetching page 28...\n",
            "üîé Fetching page 29...\n",
            "üîé Fetching page 30...\n",
            "‚úÖ Found 0 unique recipe URLs.\n",
            "\n",
            "‚úÖ Scraped 0 recipes and saved to 'tasty_recipes.csv'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1ac39d60-3c79-4e5b-9a82-87068721085c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1ac39d60-3c79-4e5b-9a82-87068721085c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1ac39d60-3c79-4e5b-9a82-87068721085c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1ac39d60-3c79-4e5b-9a82-87068721085c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 0,\n  \"fields\": []\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Replace with your actual app_id and app_key\n",
        "app_id = 'your_app_id'\n",
        "app_key = 'your_app_key'\n",
        "\n",
        "# Define health conditions and corresponding Edamam health labels\n",
        "health_conditions = {\n",
        "    'diabetes': ['low-sugar'],\n",
        "    'hypertension': ['low-sodium'],\n",
        "    'celiac': ['gluten-free'],\n",
        "    'kidney_disease': ['low-potassium'],\n",
        "    'heart_disease': ['low-fat']\n",
        "}\n",
        "\n",
        "# Initialize an empty list to store recipe data\n",
        "all_recipes = []\n",
        "\n",
        "# Iterate over each health condition\n",
        "for condition, labels in health_conditions.items():\n",
        "    for label in labels:\n",
        "        print(f\"Fetching recipes for {condition} with label {label}...\")\n",
        "        url = 'https://api.edamam.com/search'\n",
        "        params = {\n",
        "            'q': '',  # Empty query to get a broad range of recipes\n",
        "            'app_id': app_id,\n",
        "            'app_key': app_key,\n",
        "            'health': label,\n",
        "            'from': 0,\n",
        "            'to': 100  # Adjust as needed; max 100 per request\n",
        "        }\n",
        "        response = requests.get(url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            for hit in data['hits']:\n",
        "                recipe = hit['recipe']\n",
        "                all_recipes.append({\n",
        "                    'Title': recipe.get('label'),\n",
        "                    'Ingredients': recipe.get('ingredientLines'),\n",
        "                    'Calories': recipe.get('calories'),\n",
        "                    'Total Weight': recipe.get('totalWeight'),\n",
        "                    'Diet Labels': recipe.get('dietLabels'),\n",
        "                    'Health Labels': recipe.get('healthLabels'),\n",
        "                    'Cuisine Type': recipe.get('cuisineType'),\n",
        "                    'Meal Type': recipe.get('mealType'),\n",
        "                    'Dish Type': recipe.get('dishType'),\n",
        "                    'URL': recipe.get('url')\n",
        "                })\n",
        "        else:\n",
        "            print(f\"Failed to fetch recipes for {condition} with label {label}. Status code: {response.status_code}\")\n",
        "        time.sleep(1)  # To respect API rate limits\n",
        "\n",
        "# Convert the list to a DataFrame\n",
        "df = pd.DataFrame(all_recipes)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv('nutrifusion_recipes.csv', index=False)\n",
        "print(\"Recipes saved to 'nutrifusion_recipes.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBulq1v50QJJ",
        "outputId": "b3e66514-b6aa-4d64-bd60-e85cdfa63b48"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching recipes for diabetes with label low-sugar...\n",
            "Failed to fetch recipes for diabetes with label low-sugar. Status code: 404\n",
            "Fetching recipes for hypertension with label low-sodium...\n",
            "Failed to fetch recipes for hypertension with label low-sodium. Status code: 404\n",
            "Fetching recipes for celiac with label gluten-free...\n",
            "Failed to fetch recipes for celiac with label gluten-free. Status code: 404\n",
            "Fetching recipes for kidney_disease with label low-potassium...\n",
            "Failed to fetch recipes for kidney_disease with label low-potassium. Status code: 404\n",
            "Fetching recipes for heart_disease with label low-fat...\n",
            "Failed to fetch recipes for heart_disease with label low-fat. Status code: 404\n",
            "Recipes saved to 'nutrifusion_recipes.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 0: Install dependencies (for Google Colab or local)\n",
        "!pip install requests pandas\n",
        "\n",
        "# STEP 1: Import libraries\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# STEP 2: Add your Edamam credentials here\n",
        "app_id = '59456ca4'  # <-- Replace this\n",
        "app_key = 'ebeb4beaff17d1a14fcec62a904206a3'  # <-- Replace this\n",
        "\n",
        "# STEP 3: Define health conditions and supported Edamam labels\n",
        "health_conditions = {\n",
        "    'diabetes': ['low-sugar', 'sugar-conscious'],\n",
        "    'hypertension': ['low-fat-abs'],\n",
        "    'celiac': ['gluten-free'],\n",
        "    'kidney_disease': ['kidney-friendly', 'low-potassium'],\n",
        "    'heart_disease': ['low-fat-abs', 'no-oil-added']\n",
        "}\n",
        "\n",
        "# STEP 4: Create an empty list for all recipes\n",
        "all_recipes = []\n",
        "\n",
        "# STEP 5: Loop through conditions and fetch recipes\n",
        "for condition, labels in health_conditions.items():\n",
        "    for label in labels:\n",
        "        print(f\"üîç Fetching recipes for {condition} using label: {label}\")\n",
        "\n",
        "        for start in range(0, 100, 20):  # Edamam returns max 100; 20 per batch\n",
        "            url = 'https://api.edamam.com/search'\n",
        "            params = {\n",
        "                'q': '',  # General query for variety\n",
        "                'app_id': app_id,\n",
        "                'app_key': app_key,\n",
        "                'health': label,\n",
        "                'from': start,\n",
        "                'to': start + 20\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, params=params)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                for hit in data['hits']:\n",
        "                    recipe = hit['recipe']\n",
        "                    all_recipes.append({\n",
        "                        'Condition': condition,\n",
        "                        'Title': recipe.get('label'),\n",
        "                        'Ingredients': recipe.get('ingredientLines'),\n",
        "                        'Calories': recipe.get('calories'),\n",
        "                        'Total Weight': recipe.get('totalWeight'),\n",
        "                        'Diet Labels': recipe.get('dietLabels'),\n",
        "                        'Health Labels': recipe.get('healthLabels'),\n",
        "                        'Cuisine Type': recipe.get('cuisineType'),\n",
        "                        'Meal Type': recipe.get('mealType'),\n",
        "                        'Dish Type': recipe.get('dishType'),\n",
        "                        'URL': recipe.get('url')\n",
        "                    })\n",
        "            else:\n",
        "                print(f\"‚ùå Failed to fetch recipes for {condition} with label {label}. Status code: {response.status_code}\")\n",
        "            time.sleep(1)  # To avoid hitting rate limit\n",
        "\n",
        "# STEP 6: Save results\n",
        "df = pd.DataFrame(all_recipes)\n",
        "df.to_csv('nutrifusion_recipes.csv', index=False)\n",
        "\n",
        "# STEP 7: Output summary\n",
        "print(f\"‚úÖ Scraped {len(df)} recipes and saved to 'nutrifusion_recipes.csv'\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "g035s7zK0y3a",
        "outputId": "4e75d98d-6632-4f32-8905-14fc0e22edfa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "üîç Fetching recipes for diabetes using label: low-sugar\n",
            "‚ùå Failed to fetch recipes for diabetes with label low-sugar. Status code: 404\n",
            "‚ùå Failed to fetch recipes for diabetes with label low-sugar. Status code: 404\n",
            "‚ùå Failed to fetch recipes for diabetes with label low-sugar. Status code: 404\n",
            "‚ùå Failed to fetch recipes for diabetes with label low-sugar. Status code: 404\n",
            "‚ùå Failed to fetch recipes for diabetes with label low-sugar. Status code: 404\n",
            "üîç Fetching recipes for diabetes using label: sugar-conscious\n",
            "‚ùå Failed to fetch recipes for diabetes with label sugar-conscious. Status code: 404\n",
            "‚ùå Failed to fetch recipes for diabetes with label sugar-conscious. Status code: 404\n",
            "‚ùå Failed to fetch recipes for diabetes with label sugar-conscious. Status code: 404\n",
            "‚ùå Failed to fetch recipes for diabetes with label sugar-conscious. Status code: 404\n",
            "‚ùå Failed to fetch recipes for diabetes with label sugar-conscious. Status code: 404\n",
            "üîç Fetching recipes for hypertension using label: low-fat-abs\n",
            "‚ùå Failed to fetch recipes for hypertension with label low-fat-abs. Status code: 404\n",
            "‚ùå Failed to fetch recipes for hypertension with label low-fat-abs. Status code: 404\n",
            "‚ùå Failed to fetch recipes for hypertension with label low-fat-abs. Status code: 404\n",
            "‚ùå Failed to fetch recipes for hypertension with label low-fat-abs. Status code: 404\n",
            "‚ùå Failed to fetch recipes for hypertension with label low-fat-abs. Status code: 404\n",
            "üîç Fetching recipes for celiac using label: gluten-free\n",
            "‚ùå Failed to fetch recipes for celiac with label gluten-free. Status code: 404\n",
            "‚ùå Failed to fetch recipes for celiac with label gluten-free. Status code: 404\n",
            "‚ùå Failed to fetch recipes for celiac with label gluten-free. Status code: 404\n",
            "‚ùå Failed to fetch recipes for celiac with label gluten-free. Status code: 404\n",
            "‚ùå Failed to fetch recipes for celiac with label gluten-free. Status code: 404\n",
            "üîç Fetching recipes for kidney_disease using label: kidney-friendly\n",
            "‚ùå Failed to fetch recipes for kidney_disease with label kidney-friendly. Status code: 404\n",
            "‚ùå Failed to fetch recipes for kidney_disease with label kidney-friendly. Status code: 404\n",
            "‚ùå Failed to fetch recipes for kidney_disease with label kidney-friendly. Status code: 404\n",
            "‚ùå Failed to fetch recipes for kidney_disease with label kidney-friendly. Status code: 404\n",
            "‚ùå Failed to fetch recipes for kidney_disease with label kidney-friendly. Status code: 404\n",
            "üîç Fetching recipes for kidney_disease using label: low-potassium\n",
            "‚ùå Failed to fetch recipes for kidney_disease with label low-potassium. Status code: 404\n",
            "‚ùå Failed to fetch recipes for kidney_disease with label low-potassium. Status code: 404\n",
            "‚ùå Failed to fetch recipes for kidney_disease with label low-potassium. Status code: 404\n",
            "‚ùå Failed to fetch recipes for kidney_disease with label low-potassium. Status code: 404\n",
            "‚ùå Failed to fetch recipes for kidney_disease with label low-potassium. Status code: 404\n",
            "üîç Fetching recipes for heart_disease using label: low-fat-abs\n",
            "‚ùå Failed to fetch recipes for heart_disease with label low-fat-abs. Status code: 404\n",
            "‚ùå Failed to fetch recipes for heart_disease with label low-fat-abs. Status code: 404\n",
            "‚ùå Failed to fetch recipes for heart_disease with label low-fat-abs. Status code: 404\n",
            "‚ùå Failed to fetch recipes for heart_disease with label low-fat-abs. Status code: 404\n",
            "‚ùå Failed to fetch recipes for heart_disease with label low-fat-abs. Status code: 404\n",
            "üîç Fetching recipes for heart_disease using label: no-oil-added\n",
            "‚ùå Failed to fetch recipes for heart_disease with label no-oil-added. Status code: 404\n",
            "‚ùå Failed to fetch recipes for heart_disease with label no-oil-added. Status code: 404\n",
            "‚ùå Failed to fetch recipes for heart_disease with label no-oil-added. Status code: 404\n",
            "‚ùå Failed to fetch recipes for heart_disease with label no-oil-added. Status code: 404\n",
            "‚ùå Failed to fetch recipes for heart_disease with label no-oil-added. Status code: 404\n",
            "‚úÖ Scraped 0 recipes and saved to 'nutrifusion_recipes.csv'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: []\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5297aec-d469-4c8d-b157-73d584354056\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5297aec-d469-4c8d-b157-73d584354056')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f5297aec-d469-4c8d-b157-73d584354056 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f5297aec-d469-4c8d-b157-73d584354056');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 0,\n  \"fields\": []\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install requests pandas\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Step 3: Set your Spoonacular API key\n",
        "api_key = '816057ea03c2490ab4329875887ce57b'  # <-- Replace with your key\n",
        "\n",
        "# Step 4: Function to fetch detailed recipe info by ID\n",
        "def get_recipe_info(recipe_id):\n",
        "    url = f\"https://api.spoonacular.com/recipes/{recipe_id}/information\"\n",
        "    params = {\"includeNutrition\": True, \"apiKey\": api_key}\n",
        "    res = requests.get(url, params=params)\n",
        "    if res.status_code == 200:\n",
        "        return res.json()\n",
        "    return None\n",
        "\n",
        "# Step 5: Fetch 300+ recipes using bulk search\n",
        "all_recipes = []\n",
        "offset = 0\n",
        "while len(all_recipes) < 300:\n",
        "    url = \"https://api.spoonacular.com/recipes/complexSearch\"\n",
        "    params = {\n",
        "        \"apiKey\": api_key,\n",
        "        \"number\": 50,\n",
        "        \"offset\": offset,\n",
        "        \"addRecipeNutrition\": True,\n",
        "    }\n",
        "    res = requests.get(url, params=params)\n",
        "    data = res.json()\n",
        "\n",
        "    if 'results' not in data:\n",
        "        print(\"‚ùå Error fetching data:\", data)\n",
        "        break\n",
        "\n",
        "    for item in data['results']:\n",
        "        recipe = get_recipe_info(item['id'])\n",
        "        if recipe:\n",
        "            all_recipes.append({\n",
        "                'ID': recipe['id'],\n",
        "                'Title': recipe['title'],\n",
        "                'ReadyInMinutes': recipe.get('readyInMinutes'),\n",
        "                'Servings': recipe.get('servings'),\n",
        "                'Vegetarian': recipe.get('vegetarian'),\n",
        "                'Vegan': recipe.get('vegan'),\n",
        "                'GlutenFree': recipe.get('glutenFree'),\n",
        "                'DairyFree': recipe.get('dairyFree'),\n",
        "                'Ingredients': [ing['original'] for ing in recipe.get('extendedIngredients', [])],\n",
        "                'Calories': recipe.get('nutrition', {}).get('nutrients', [{}])[0].get('amount'),\n",
        "                'Protein': next((n['amount'] for n in recipe['nutrition']['nutrients'] if n['title'] == 'Protein'), None),\n",
        "                'Fat': next((n['amount'] for n in recipe['nutrition']['nutrients'] if n['title'] == 'Fat'), None),\n",
        "                'Carbohydrates': next((n['amount'] for n in recipe['nutrition']['nutrients'] if n['title'] == 'Carbohydrates'), None),\n",
        "                'Source URL': recipe.get('sourceUrl')\n",
        "            })\n",
        "    offset += 50\n",
        "    time.sleep(1)  # Respect API rate limits\n",
        "\n",
        "# Step 6: Save to CSV\n",
        "df = pd.DataFrame(all_recipes)\n",
        "df.to_csv(\"nutrifusion_recipes.csv\", index=False)\n",
        "print(f\"‚úÖ Saved {len(df)} recipes to 'nutrifusion_recipes.csv'\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "id": "-8q1AVh82wPE",
        "outputId": "3a66adbe-7aab-4f68-e4b9-22cb5eb5c610"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'title'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d28e4200c110>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;34m'Ingredients'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ming\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'original'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ming\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'extendedIngredients'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;34m'Calories'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nutrition'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nutrients'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'amount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0;34m'Protein'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nutrition'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nutrients'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Protein'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0;34m'Fat'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nutrition'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nutrients'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Fat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;34m'Carbohydrates'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nutrition'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nutrients'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Carbohydrates'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-d28e4200c110>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;34m'Ingredients'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ming\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'original'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ming\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'extendedIngredients'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;34m'Calories'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nutrition'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nutrients'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'amount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0;34m'Protein'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nutrition'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nutrients'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Protein'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0;34m'Fat'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nutrition'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nutrients'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Fat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;34m'Carbohydrates'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amount'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nutrition'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nutrients'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Carbohydrates'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'title'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install required packages\n",
        "!pip install requests pandas\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Step 3: Set your Spoonacular API key\n",
        "api_key = '816057ea03c2490ab4329875887ce57b'  # <-- Replace with your key\n",
        "\n",
        "# Step 4: Function to fetch detailed recipe info by ID\n",
        "def get_recipe_info(recipe_id):\n",
        "    url = f\"https://api.spoonacular.com/recipes/{recipe_id}/information\"\n",
        "    params = {\"includeNutrition\": True, \"apiKey\": api_key}\n",
        "    res = requests.get(url, params=params)\n",
        "    if res.status_code == 200:\n",
        "        return res.json()\n",
        "    return None\n",
        "\n",
        "# Step 5: Fetch 300+ recipes using bulk search\n",
        "all_recipes = []\n",
        "offset = 0\n",
        "while len(all_recipes) < 300:\n",
        "    url = \"https://api.spoonacular.com/recipes/complexSearch\"\n",
        "    params = {\n",
        "        \"apiKey\": api_key,\n",
        "        \"number\": 50,\n",
        "        \"offset\": offset,\n",
        "        \"addRecipeNutrition\": True,\n",
        "    }\n",
        "    res = requests.get(url, params=params)\n",
        "    data = res.json()\n",
        "\n",
        "    if 'results' not in data:\n",
        "        print(\"‚ùå Error fetching data:\", data)\n",
        "        break\n",
        "\n",
        "    for item in data['results']:\n",
        "        recipe = get_recipe_info(item['id'])\n",
        "        if recipe:\n",
        "            # Access nutrients safely\n",
        "            nutrients = recipe.get('nutrition', {}).get('nutrients', [])\n",
        "\n",
        "            all_recipes.append({\n",
        "                'ID': recipe['id'],\n",
        "                'Title': recipe['title'],\n",
        "                'ReadyInMinutes': recipe.get('readyInMinutes'),\n",
        "                'Servings': recipe.get('servings'),\n",
        "                'Vegetarian': recipe.get('vegetarian'),\n",
        "                'Vegan': recipe.get('vegan'),\n",
        "                'GlutenFree': recipe.get('glutenFree'),\n",
        "                'DairyFree': recipe.get('dairyFree'),\n",
        "                'Ingredients': [ing['original'] for ing in recipe.get('extendedIngredients', [])],\n",
        "                'Calories': next((n.get('amount') for n in nutrients if n.get('title') == 'Calories'), None), # Also update calories for consistency\n",
        "                'Protein': next((n.get('amount') for n in nutrients if n.get('title') == 'Protein'), None),\n",
        "                'Fat': next((n.get('amount') for n in nutrients if n.get('title') == 'Fat'), None),\n",
        "                'Carbohydrates': next((n.get('amount') for n in nutrients if n.get('title') == 'Carbohydrates'), None),\n",
        "                'Source URL': recipe.get('sourceUrl')\n",
        "            })\n",
        "    offset += 50\n",
        "    time.sleep(1)  # Respect API rate limits\n",
        "\n",
        "# Step 6: Save to CSV\n",
        "df = pd.DataFrame(all_recipes)\n",
        "df.to_csv(\"nutrifusion_recipes.csv\", index=False)\n",
        "print(f\"‚úÖ Saved {len(df)} recipes to 'nutrifusion_recipes.csv'\")\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "id": "qaF1YCqO3nYd",
        "outputId": "51374e90-0069-4805-9c53-8a58ba7b95a2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "‚ùå Error fetching data: {'status': 'failure', 'code': 402, 'message': 'Your daily points limit of 150 has been reached. Please upgrade your plan to continue using the API.'}\n",
            "‚úÖ Saved 117 recipes to 'nutrifusion_recipes.csv'\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       ID                                          Title  ReadyInMinutes  \\\n",
              "0  715415       Red Lentil Soup with Chicken and Turnips              55   \n",
              "1  716406  Asparagus and Pea Soup: Real Convenience Food              20   \n",
              "2  644387                                  Garlicky Kale              45   \n",
              "3  715446                          Slow Cooker Beef Stew             490   \n",
              "4  782601                      Red Kidney Bean Jambalaya              45   \n",
              "\n",
              "   Servings  Vegetarian  Vegan  GlutenFree  DairyFree  \\\n",
              "0         8       False  False        True       True   \n",
              "1         2        True   True        True       True   \n",
              "2         2        True   True        True       True   \n",
              "3         6       False  False        True       True   \n",
              "4         6        True   True        True       True   \n",
              "\n",
              "                                         Ingredients Calories Protein   Fat  \\\n",
              "0  [additional toppings: diced avocado, micro gre...     None    None  None   \n",
              "1  [1 bag of frozen organic asparagus (preferably...     None    None  None   \n",
              "2  [3 tablespoons balsamic vinegar, 1 clove garli...     None    None  None   \n",
              "3  [1 14.5oz can of Beef Broth, 2 large carrots, ...     None    None  None   \n",
              "4  [2/3 cup dried brown rice (2 cups cooked), 2 m...     None    None  None   \n",
              "\n",
              "  Carbohydrates                                         Source URL  \n",
              "0          None  https://www.pinkwhen.com/red-lentil-soup-with-...  \n",
              "1          None  https://fullbellysisters.blogspot.com/2011/03/...  \n",
              "2          None  https://www.foodista.com/recipe/J2FTJBF7/garli...  \n",
              "3          None  https://www.pinkwhen.com/slow-cooker-beef-stew...  \n",
              "4          None  https://www.foodandspice.com/2016/05/red-kidne...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-91b089bd-8984-4f11-a6de-92607701084c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Title</th>\n",
              "      <th>ReadyInMinutes</th>\n",
              "      <th>Servings</th>\n",
              "      <th>Vegetarian</th>\n",
              "      <th>Vegan</th>\n",
              "      <th>GlutenFree</th>\n",
              "      <th>DairyFree</th>\n",
              "      <th>Ingredients</th>\n",
              "      <th>Calories</th>\n",
              "      <th>Protein</th>\n",
              "      <th>Fat</th>\n",
              "      <th>Carbohydrates</th>\n",
              "      <th>Source URL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>715415</td>\n",
              "      <td>Red Lentil Soup with Chicken and Turnips</td>\n",
              "      <td>55</td>\n",
              "      <td>8</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>[additional toppings: diced avocado, micro gre...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>https://www.pinkwhen.com/red-lentil-soup-with-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>716406</td>\n",
              "      <td>Asparagus and Pea Soup: Real Convenience Food</td>\n",
              "      <td>20</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>[1 bag of frozen organic asparagus (preferably...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>https://fullbellysisters.blogspot.com/2011/03/...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>644387</td>\n",
              "      <td>Garlicky Kale</td>\n",
              "      <td>45</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>[3 tablespoons balsamic vinegar, 1 clove garli...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>https://www.foodista.com/recipe/J2FTJBF7/garli...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>715446</td>\n",
              "      <td>Slow Cooker Beef Stew</td>\n",
              "      <td>490</td>\n",
              "      <td>6</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>[1 14.5oz can of Beef Broth, 2 large carrots, ...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>https://www.pinkwhen.com/slow-cooker-beef-stew...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>782601</td>\n",
              "      <td>Red Kidney Bean Jambalaya</td>\n",
              "      <td>45</td>\n",
              "      <td>6</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>[2/3 cup dried brown rice (2 cups cooked), 2 m...</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>https://www.foodandspice.com/2016/05/red-kidne...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91b089bd-8984-4f11-a6de-92607701084c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-91b089bd-8984-4f11-a6de-92607701084c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-91b089bd-8984-4f11-a6de-92607701084c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-656db492-3a2e-43d6-b499-64427529e2c5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-656db492-3a2e-43d6-b499-64427529e2c5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-656db492-3a2e-43d6-b499-64427529e2c5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
        "}\n",
        "\n",
        "def get_recipe_links(base_url, max_links=10):\n",
        "    print(\"Fetching recipe links...\")\n",
        "    links = set()\n",
        "    page = 1\n",
        "\n",
        "    while len(links) < max_links:\n",
        "        url = f\"{base_url}?page={page}\"\n",
        "        res = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "        for a in soup.select('a.card__titleLink'):\n",
        "            href = a.get('href')\n",
        "            if href and '/recipe/' in href:\n",
        "                links.add(href)\n",
        "                if len(links) >= max_links:\n",
        "                    break\n",
        "        page += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "    return list(links)\n",
        "\n",
        "def scrape_allrecipes(url):\n",
        "    try:\n",
        "        res = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(res.content, 'html.parser')\n",
        "        script_tag = soup.find('script', type='application/ld+json')\n",
        "        if not script_tag:\n",
        "            return None\n",
        "\n",
        "        data = json.loads(script_tag.string)\n",
        "        if isinstance(data, list):\n",
        "            data = data[0]\n",
        "        nutrition = data.get('nutrition', {})\n",
        "        ingredients = data.get('recipeIngredient', [])\n",
        "\n",
        "        return {\n",
        "            \"Name\": data.get('name'),\n",
        "            \"Ingredients\": \", \".join(ingredients),\n",
        "            \"Calories\": nutrition.get('calories'),\n",
        "            \"Fat\": nutrition.get('fatContent'),\n",
        "            \"Carbohydrates\": nutrition.get('carbohydrateContent'),\n",
        "            \"Protein\": nutrition.get('proteinContent'),\n",
        "            \"URL\": url\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to scrape {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# üîó Set category page to fetch from (e.g. \"dinner\", \"chicken\", etc.)\n",
        "base_url = \"https://www.allrecipes.com/recipes/201/meat-and-poultry/chicken/\"  # you can change this\n",
        "max_recipes = 10  # number of recipes to scrape\n",
        "\n",
        "recipe_urls = get_recipe_links(base_url, max_recipes)\n",
        "recipes = []\n",
        "\n",
        "for url in recipe_urls:\n",
        "    print(f\"Scraping: {url}\")\n",
        "    data = scrape_allrecipes(url)\n",
        "    if data:\n",
        "        recipes.append(data)\n",
        "    time.sleep(1)\n",
        "\n",
        "df = pd.DataFrame(recipes)\n",
        "df.to_excel(\"allrecipes_nutrifusion_dataset.xlsx\", index=False)\n",
        "\n",
        "print(\"‚úÖ Done! Data saved to 'allrecipes_nutrifusion_dataset.xlsx'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3C-nQ0j7xGT",
        "outputId": "c71aed2a-9494-4231-bd9c-2807f328dc62"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching recipe links...\n"
          ]
        }
      ]
    }
  ]
}