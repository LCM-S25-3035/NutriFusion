{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aEE8K9eAwM4"
      },
      "outputs": [],
      "source": [
        "pip install requests beautifulsoup4 pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium\n"
      ],
      "metadata": {
        "id": "jn76FPvRDO4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install webdriver-manager\n"
      ],
      "metadata": {
        "id": "E_1xonXcSdoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n"
      ],
      "metadata": {
        "id": "8VTx6hAXQR_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/usr/lib/chromium-browser/chromedriver')\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import csv\n"
      ],
      "metadata": {
        "id": "zOgjPd4IQjzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "options = Options()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--disable-gpu')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "driver = webdriver.Chrome(options=options)\n"
      ],
      "metadata": {
        "id": "z2-bkGWvQlBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_recipe_links_selenium(url):\n",
        "    driver.get(url)\n",
        "    time.sleep(2)  # wait for page to load\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    recipe_cards = soup.select('a.card__titleLink')\n",
        "    return [card['href'] for card in recipe_cards if 'href' in card.attrs]\n",
        "\n",
        "def parse_recipe_selenium(recipe_url):\n",
        "    try:\n",
        "        driver.get(recipe_url)\n",
        "        time.sleep(2)\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "        title = soup.find('h1', class_='headline heading-content')\n",
        "        ingredients = [i.get_text(strip=True) for i in soup.select('span.ingredients-item-name')]\n",
        "        directions = [d.get_text(strip=True) for d in soup.select('li.subcontainer.instructions-section-item p')]\n",
        "\n",
        "        return {\n",
        "            'title': title.text.strip() if title else 'No title',\n",
        "            'ingredients': ingredients,\n",
        "            'directions': directions,\n",
        "            'url': recipe_url\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "dFchq1JUQqCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_allrecipes_selenium(pages=1):\n",
        "    base_url = \"https://www.allrecipes.com/recipes/201/meat-and-poultry/chicken/?page=\"\n",
        "    all_recipes = []\n",
        "    for page in range(1, pages+1):\n",
        "        print(f\"Scraping page {page}\")\n",
        "        links = get_recipe_links_selenium(base_url + str(page))\n",
        "        print(f\"Found {len(links)} recipes\")\n",
        "\n",
        "        for link in links:\n",
        "            recipe = parse_recipe_selenium(link)\n",
        "            if recipe:\n",
        "                all_recipes.append(recipe)\n",
        "            time.sleep(1)\n",
        "    return all_recipes\n",
        "\n",
        "def save_to_csv(recipes, filename='chicken_recipes.csv'):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=['title', 'ingredients', 'directions', 'url'])\n",
        "        writer.writeheader()\n",
        "        for r in recipes:\n",
        "            writer.writerow({\n",
        "                'title': r['title'],\n",
        "                'ingredients': '; '.join(r['ingredients']),\n",
        "                'directions': ' '.join(r['directions']),\n",
        "                'url': r['url']\n",
        "            })\n"
      ],
      "metadata": {
        "id": "gRGRPpddQwTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_allrecipes_selenium(pages=1):\n",
        "    from selenium import webdriver\n",
        "    from selenium.webdriver.chrome.options import Options\n",
        "    from selenium.webdriver.common.by import By\n",
        "    from selenium.webdriver.support.ui import WebDriverWait\n",
        "    from selenium.webdriver.support import expected_conditions as EC\n",
        "    from selenium.common.exceptions import TimeoutException\n",
        "    from selenium.webdriver.chrome.service import Service\n",
        "    from webdriver_manager.chrome import ChromeDriverManager\n",
        "    import time\n",
        "\n",
        "    options = Options()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "\n",
        "    base_url = \"https://www.allrecipes.com/recipes/201/meat-and-poultry/chicken/?page=\"\n",
        "    all_recipes = []\n",
        "\n",
        "    for page in range(1, pages + 1):\n",
        "        print(f\"Scraping page {page}\")\n",
        "        driver.get(base_url + str(page))\n",
        "        time.sleep(3)  # wait for JS to load\n",
        "\n",
        "        # Close cookie popup if present\n",
        "        try:\n",
        "            consent_button = WebDriverWait(driver, 5).until(\n",
        "                EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
        "            )\n",
        "            consent_button.click()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        recipe_links = driver.find_elements(By.CSS_SELECTOR, \"a.comp.card__titleLink\")\n",
        "\n",
        "        urls = [link.get_attribute(\"href\") for link in recipe_links if link.get_attribute(\"href\")]\n",
        "        print(f\"Found {len(urls)} recipes\")\n",
        "\n",
        "        for url in urls:\n",
        "            try:\n",
        "                driver.get(url)\n",
        "                time.sleep(2)\n",
        "\n",
        "                title = driver.find_element(By.CSS_SELECTOR, \"h1\").text.strip()\n",
        "\n",
        "                ingredients_elements = driver.find_elements(By.CSS_SELECTOR, \"span.ingredients-item-name\")\n",
        "                ingredients = [elem.text.strip() for elem in ingredients_elements]\n",
        "\n",
        "                directions_elements = driver.find_elements(By.CSS_SELECTOR, \"li.subcontainer.instructions-section-item > div.section-body\")\n",
        "                directions = [elem.text.strip() for elem in directions_elements]\n",
        "\n",
        "                all_recipes.append({\n",
        "                    \"title\": title,\n",
        "                    \"ingredients\": \"; \".join(ingredients),\n",
        "                    \"directions\": \". \".join(directions),\n",
        "                    \"url\": url\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to scrape {url}: {e}\")\n",
        "                continue\n",
        "\n",
        "    driver.quit()\n",
        "    return all_recipes\n"
      ],
      "metadata": {
        "id": "BvE-e4DuR-NK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_to_csv(data, filename='chicken_recipes.csv'):\n",
        "    import pandas as pd\n",
        "    df = pd.DataFrame(data)\n",
        "    df.to_csv(filename, index=False)\n"
      ],
      "metadata": {
        "id": "Tui3l_QmSAsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import csv\n",
        "\n",
        "HEADERS = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                  \"Chrome/114.0.0.0 Safari/537.36\",\n",
        "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
        "    \"Referer\": \"https://www.google.com/\",\n",
        "    \"Connection\": \"keep-alive\",\n",
        "    \"Upgrade-Insecure-Requests\": \"1\",\n",
        "}\n",
        "\n",
        "BASE_URL = \"https://www.allrecipes.com/recipes/201/meat-and-poultry/chicken/?page={}\"\n",
        "\n",
        "def scrape_recipes(num_pages=20):\n",
        "    session = requests.Session()\n",
        "    session.headers.update(HEADERS)\n",
        "\n",
        "    recipes = []\n",
        "    for page in range(1, num_pages + 1):\n",
        "        print(f\"Scraping page {page}...\")\n",
        "        url = BASE_URL.format(page)\n",
        "        try:\n",
        "            response = session.get(url)\n",
        "            response.raise_for_status()\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Error fetching page {page}: {e}\")\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        cards = soup.select(\"article.fixed-recipe-card\") or soup.select(\"div.card__detailsContainer\")\n",
        "\n",
        "        print(f\"Found {len(cards)} recipes on page {page}\")\n",
        "\n",
        "        for card in cards:\n",
        "            title_tag = card.select_one(\"h3.fixed-recipe-card__h3\") or card.select_one(\"h3.card__title\")\n",
        "            link_tag = card.select_one(\"a.fixed-recipe-card__title-link\") or card.select_one(\"a.card__titleLink\")\n",
        "\n",
        "            if title_tag and link_tag:\n",
        "                title = title_tag.get_text(strip=True)\n",
        "                link = link_tag.get(\"href\")\n",
        "                recipes.append({\"title\": title, \"url\": link})\n",
        "\n",
        "        time.sleep(3)  # longer delay to avoid blocking\n",
        "\n",
        "    return recipes\n",
        "\n",
        "def save_to_csv(recipes, filename=\"chicken_recipes.csv\"):\n",
        "    with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"title\", \"url\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(recipes)\n",
        "    print(f\"Saved {len(recipes)} recipes to {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    recipes = scrape_recipes(num_pages=20)\n",
        "    save_to_csv(recipes)\n"
      ],
      "metadata": {
        "id": "jrR8GmESVqwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-allrecipes==0.3.1\n"
      ],
      "metadata": {
        "id": "_BBnhFFhWXFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_tasty_recipes(pages=1, output_file=\"tasty_recipes.csv\"):\n",
        "    base_url = \"https://tasty.co/topic/chicken\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0\"\n",
        "    }\n",
        "\n",
        "    recipe_data = []\n",
        "\n",
        "    print(\"Scraping Tasty.co...\")\n",
        "\n",
        "    response = requests.get(base_url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to load page: {base_url}\")\n",
        "        return\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    links = soup.select('a.feed-item analyt-unit-tap')[:10]  # Top 10 recipes on the page\n",
        "\n",
        "    for link in links:\n",
        "        recipe_url = \"https://tasty.co\" + link['href']\n",
        "        print(f\"Fetching: {recipe_url}\")\n",
        "        recipe_resp = requests.get(recipe_url, headers=headers)\n",
        "        if recipe_resp.status_code != 200:\n",
        "            print(f\"Failed to fetch: {recipe_url}\")\n",
        "            continue\n",
        "\n",
        "        recipe_soup = BeautifulSoup(recipe_resp.content, 'html.parser')\n",
        "\n",
        "        title_tag = recipe_soup.find('h1', class_='recipe-name')\n",
        "        ingredients = recipe_soup.select('li.ingredient')\n",
        "        steps = recipe_soup.select('li.preparation-step')\n",
        "\n",
        "        title = title_tag.text.strip() if title_tag else \"N/A\"\n",
        "        ingredient_list = [i.text.strip() for i in ingredients]\n",
        "        instruction_list = [s.text.strip() for s in steps]\n",
        "\n",
        "        recipe_data.append({\n",
        "            'title': title,\n",
        "            'ingredients': \" | \".join(ingredient_list),\n",
        "            'instructions': \" | \".join(instruction_list)\n",
        "        })\n",
        "\n",
        "    # Save to CSV\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"title\", \"ingredients\", \"instructions\"])\n",
        "        writer.writeheader()\n",
        "        for recipe in recipe_data:\n",
        "            writer.writerow(recipe)\n",
        "\n",
        "    print(f\"\\nSaved {len(recipe_data)} recipes to {output_file}\")\n",
        "\n",
        "# Run the scraper\n",
        "scrape_tasty_recipes()\n"
      ],
      "metadata": {
        "id": "-TVUxHpEYZOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "\n",
        "def get_mealdb_all_recipes(max_limit=10000, output_csv=\"mealdb_all_recipes.csv\"):\n",
        "    recipes = []\n",
        "    letters = [chr(i) for i in range(ord('a'), ord('z')+1)]\n",
        "\n",
        "    for letter in letters:\n",
        "        url = f\"https://www.themealdb.com/api/json/v1/1/search.php?f={letter}\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch letter '{letter}'\")\n",
        "            continue\n",
        "\n",
        "        data = response.json()\n",
        "        meals = data.get(\"meals\")\n",
        "        if not meals:\n",
        "            continue\n",
        "\n",
        "        for meal in meals:\n",
        "            title = meal.get(\"strMeal\")\n",
        "            instructions = meal.get(\"strInstructions\")\n",
        "\n",
        "            ingredients = []\n",
        "            for i in range(1, 21):\n",
        "                ingredient = meal.get(f\"strIngredient{i}\")\n",
        "                measure = meal.get(f\"strMeasure{i}\")\n",
        "                if ingredient and ingredient.strip():\n",
        "                    ingredients.append(f\"{measure.strip()} {ingredient.strip()}\")\n",
        "\n",
        "            recipes.append({\n",
        "                \"title\": title,\n",
        "                \"ingredients\": \" | \".join(ingredients),\n",
        "                \"instructions\": instructions\n",
        "            })\n",
        "\n",
        "            if len(recipes) >= max_limit:\n",
        "                break\n",
        "        if len(recipes) >= max_limit:\n",
        "            break\n",
        "\n",
        "    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"title\", \"ingredients\", \"instructions\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(recipes)\n",
        "\n",
        "    print(f\"Saved {len(recipes)} recipes to '{output_csv}'\")\n",
        "\n",
        "# Run to collect up to 10,000 recipes (or fewer if not available)\n",
        "get_mealdb_all_recipes(max_limit=10000)\n"
      ],
      "metadata": {
        "id": "q8HlCAHBZoGj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}