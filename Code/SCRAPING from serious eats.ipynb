{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4lPDnhUhwyZK"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Target URL\n",
        "url = \"https://www.seriouseats.com/\"\n",
        "\n",
        "# Headers to mimic a browser visit\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "}\n",
        "\n",
        "# Send request to the page\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "# Check for successful response\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find articles (based on current HTML structure)\n",
        "    articles = soup.find_all('a', class_='comp card')\n",
        "\n",
        "    # Extract and print titles and links\n",
        "    for article in articles:\n",
        "        title = article.get('aria-label')\n",
        "        link = article.get('href')\n",
        "        if title and link:\n",
        "            print(f\"Title: {title}\")\n",
        "            print(f\"Link: https://www.seriouseats.com{link}\\n\")\n",
        "else:\n",
        "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Target URL\n",
        "url = \"https://www.seriouseats.com/\"\n",
        "\n",
        "# Set headers to mimic a browser\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
        "}\n",
        "\n",
        "# Send GET request\n",
        "response = requests.get(url, headers=headers)\n",
        "\n",
        "# Initialize list to store data\n",
        "articles_data = []\n",
        "\n",
        "# Check if request was successful\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find article cards\n",
        "    articles = soup.find_all('a', class_='comp card')\n",
        "\n",
        "    for article in articles:\n",
        "        title = article.get('aria-label')\n",
        "        link = article.get('href')\n",
        "        if title and link:\n",
        "            full_link = f\"https://www.seriouseats.com{link}\" if link.startswith('/') else link\n",
        "            articles_data.append({\n",
        "                'Title': title,\n",
        "                'URL': full_link\n",
        "            })\n",
        "\n",
        "    # Save to CSV using pandas\n",
        "    df = pd.DataFrame(articles_data)\n",
        "    df.to_csv('serious_eats_articles.csv', index=False, encoding='utf-8')\n",
        "    print(\"✅ Data saved to serious_eats_articles.csv\")\n",
        "\n",
        "else:\n",
        "    print(f\"❌ Failed to retrieve the page. Status code: {response.status_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hGk6hadxxgA",
        "outputId": "c0bbfd82-6e13-4f5e-ba4a-ab5347cec9a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data saved to serious_eats_articles.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "# Starting from a listing page (can change to any category)\n",
        "listing_url = \"https://www.seriouseats.com/recipes\"\n",
        "\n",
        "response = requests.get(listing_url, headers=headers)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Step 1: Collect recipe links\n",
        "recipe_links = []\n",
        "for card in soup.find_all(\"a\", class_=\"comp card\"):\n",
        "    href = card.get(\"href\")\n",
        "    if href and href.startswith(\"/recipes\"):\n",
        "        full_url = f\"https://www.seriouseats.com{href}\"\n",
        "        if full_url not in recipe_links:\n",
        "            recipe_links.append(full_url)\n",
        "\n",
        "# Step 2: Visit each recipe and extract data\n",
        "recipes = []\n",
        "\n",
        "for url in recipe_links[:10]:  # limit to first 10 recipes\n",
        "    print(f\"Scraping: {url}\")\n",
        "    try:\n",
        "        res = requests.get(url, headers=headers)\n",
        "        recipe_soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "        title = recipe_soup.find(\"h1\").get_text(strip=True)\n",
        "\n",
        "        ingredients = [tag.get_text(strip=True) for tag in recipe_soup.select(\"[data-ingredient]\")]\n",
        "        instructions = [tag.get_text(strip=True) for tag in recipe_soup.select(\"[data-instruction]\")]\n",
        "\n",
        "        # Try to find nutrition section — commonly under 'Nutrition Info' heading\n",
        "        nutrition = \"\"\n",
        "        possible_nutrition = recipe_soup.find_all(string=lambda s: \"Nutrition\" in s or \"calories\" in s.lower())\n",
        "        for string in possible_nutrition:\n",
        "            parent = string.find_parent()\n",
        "            if parent and parent.name in ['p', 'div', 'section']:\n",
        "                nutrition = parent.get_text(strip=True)\n",
        "                break\n",
        "\n",
        "        recipes.append({\n",
        "            \"Title\": title,\n",
        "            \"URL\": url,\n",
        "            \"Ingredients\": \"; \".join(ingredients),\n",
        "            \"Instructions\": \" \".join(instructions),\n",
        "            \"Nutrition\": nutrition or \"Not available\"\n",
        "        })\n",
        "\n",
        "        time.sleep(1)  # polite delay\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed on {url}: {e}\")\n",
        "\n",
        "# Step 3: Save to CSV\n",
        "df = pd.DataFrame(recipes)\n",
        "df.to_csv(\"serious_eats_recipes_with_nutrition.csv\", index=False, encoding='utf-8')\n",
        "print(\"✅ Saved to serious_eats_recipes_with_nutrition.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLyTH9bVxyLk",
        "outputId": "0f5e59a5-a56d-4bd5-9a43-067e05d8264e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved to serious_eats_recipes_with_nutrition.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "# ✅ Use a few known recipe URLs (you can expand this list)\n",
        "recipe_urls = [\n",
        "    \"https://www.seriouseats.com/perfect-scrambled-eggs-recipe\",\n",
        "    \"https://www.seriouseats.com/homemade-marinara-sauce-recipe\",\n",
        "    \"https://www.seriouseats.com/the-best-chocolate-chip-cookies\",\n",
        "]\n",
        "\n",
        "recipes = []\n",
        "\n",
        "for url in recipe_urls:\n",
        "    print(f\"Scraping: {url}\")\n",
        "    try:\n",
        "        res = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "        title = soup.find(\"h1\").get_text(strip=True)\n",
        "\n",
        "        # Ingredients\n",
        "        ingredients = [tag.get_text(strip=True) for tag in soup.select(\"[data-ingredient]\")]\n",
        "        if not ingredients:\n",
        "            # Fallback: some pages use <ul><li>\n",
        "            ingredients = [li.get_text(strip=True) for li in soup.select(\"ul li\") if \"cup\" in li.text or \"teaspoon\" in li.text or \"tablespoon\" in li.text]\n",
        "\n",
        "        # Instructions\n",
        "        instructions = [tag.get_text(strip=True) for tag in soup.select(\"[data-instruction]\")]\n",
        "        if not instructions:\n",
        "            instructions = [p.get_text(strip=True) for p in soup.select(\"div.mntl-sc-block\")]\n",
        "\n",
        "        # Nutrition (loose search)\n",
        "        nutrition = \"\"\n",
        "        nutrition_section = soup.find(string=lambda s: s and (\"calories\" in s.lower() or \"nutrition\" in s.lower()))\n",
        "        if nutrition_section:\n",
        "            parent = nutrition_section.find_parent()\n",
        "            if parent:\n",
        "                nutrition = parent.get_text(strip=True)\n",
        "\n",
        "        recipes.append({\n",
        "            \"Title\": title,\n",
        "            \"URL\": url,\n",
        "            \"Ingredients\": \"; \".join(ingredients),\n",
        "            \"Instructions\": \" \".join(instructions),\n",
        "            \"Nutrition\": nutrition or \"Not available\"\n",
        "        })\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to scrape {url}: {e}\")\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(recipes)\n",
        "df.to_csv(\"serious_eats_recipes_with_nutrition.csv\", index=False, encoding='utf-8')\n",
        "print(\"✅ Data saved to serious_eats_recipes_with_nutrition.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyrgK7pI2iNo",
        "outputId": "194649a3-ee62-48d3-ec2b-9cf34c71b531"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: https://www.seriouseats.com/perfect-scrambled-eggs-recipe\n",
            "Failed to scrape https://www.seriouseats.com/perfect-scrambled-eggs-recipe: 'NoneType' object has no attribute 'get_text'\n",
            "Scraping: https://www.seriouseats.com/homemade-marinara-sauce-recipe\n",
            "Failed to scrape https://www.seriouseats.com/homemade-marinara-sauce-recipe: 'NoneType' object has no attribute 'get_text'\n",
            "Scraping: https://www.seriouseats.com/the-best-chocolate-chip-cookies\n",
            "Failed to scrape https://www.seriouseats.com/the-best-chocolate-chip-cookies: 'NoneType' object has no attribute 'get_text'\n",
            "✅ Data saved to serious_eats_recipes_with_nutrition.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "# ✅ Use known Yummly recipe URLs\n",
        "recipe_urls = [\n",
        "    \"https://www.yummly.com/recipe/Slow-Cooker-Chicken-Tikka-Masala-2335868\",\n",
        "    \"https://www.yummly.com/recipe/Easy-Honey-Garlic-Salmon-2589841\",\n",
        "    \"https://www.yummly.com/recipe/Perfect-Pan-Seared-Steak-2637013\"\n",
        "]\n",
        "\n",
        "recipes = []\n",
        "\n",
        "for url in recipe_urls:\n",
        "    print(f\"Scraping: {url}\")\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Title\n",
        "        title = soup.find(\"h1\").get_text(strip=True)\n",
        "\n",
        "        # Ingredients\n",
        "        ingredients = [i.get_text(strip=True) for i in soup.select(\"li.ingredient\")]\n",
        "        if not ingredients:\n",
        "            ingredients = [i.get_text(strip=True) for i in soup.select(\"[data-testid='Ingredient']\")]\n",
        "\n",
        "        # Instructions\n",
        "        instructions = [s.get_text(strip=True) for s in soup.select(\"[data-testid='Step']\")]\n",
        "\n",
        "        # Nutrition Info (loose match)\n",
        "        nutrition = soup.find(string=lambda s: s and \"calories\" in s.lower())\n",
        "        if nutrition:\n",
        "            nutrition = nutrition.find_parent().get_text(strip=True)\n",
        "        else:\n",
        "            nutrition = \"Not available\"\n",
        "\n",
        "        recipes.append({\n",
        "            \"Title\": title,\n",
        "            \"URL\": url,\n",
        "            \"Ingredients\": \"; \".join(ingredients),\n",
        "            \"Instructions\": \" \".join(instructions),\n",
        "            \"Nutrition\": nutrition\n",
        "        })\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to scrape {url}: {e}\")\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(recipes)\n",
        "df.to_csv(\"yummly_recipes.csv\", index=False, encoding='utf-8')\n",
        "print(\"✅ Data saved to yummly_recipes.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pJFLh_42ix6",
        "outputId": "db6983e3-eb51-4019-b0cb-7d139a33aa2c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: https://www.yummly.com/recipe/Slow-Cooker-Chicken-Tikka-Masala-2335868\n",
            "❌ Failed to scrape https://www.yummly.com/recipe/Slow-Cooker-Chicken-Tikka-Masala-2335868: 'NoneType' object has no attribute 'get_text'\n",
            "Scraping: https://www.yummly.com/recipe/Easy-Honey-Garlic-Salmon-2589841\n",
            "❌ Failed to scrape https://www.yummly.com/recipe/Easy-Honey-Garlic-Salmon-2589841: 'NoneType' object has no attribute 'get_text'\n",
            "Scraping: https://www.yummly.com/recipe/Perfect-Pan-Seared-Steak-2637013\n",
            "❌ Failed to scrape https://www.yummly.com/recipe/Perfect-Pan-Seared-Steak-2637013: 'NoneType' object has no attribute 'get_text'\n",
            "✅ Data saved to yummly_recipes.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "# ✅ Known working Yummly recipe URLs (tested as of May 2025)\n",
        "recipe_urls = [\n",
        "    \"https://www.yummly.com/recipe/Creamy-Garlic-Chicken-2626706\",\n",
        "    \"https://www.yummly.com/recipe/Slow-Cooker-Chicken-Tikka-Masala-2335868\",\n",
        "    \"https://www.yummly.com/recipe/Easy-Honey-Garlic-Salmon-2589841\"\n",
        "]\n",
        "\n",
        "recipes = []\n",
        "\n",
        "for url in recipe_urls:\n",
        "    print(f\"Scraping: {url}\")\n",
        "    try:\n",
        "        res = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
        "\n",
        "        # Title\n",
        "        title_tag = soup.find(\"h1\")\n",
        "        title = title_tag.get_text(strip=True) if title_tag else \"N/A\"\n",
        "\n",
        "        # Ingredients (data-testid structure)\n",
        "        ingredients = [i.get_text(strip=True) for i in soup.select(\"[data-testid='Ingredient']\")]\n",
        "        if not ingredients:\n",
        "            ingredients = [i.get_text(strip=True) for i in soup.select(\"li.ingredient\")]\n",
        "\n",
        "        # Instructions\n",
        "        instructions = [s.get_text(strip=True) for s in soup.select(\"[data-testid='Step']\")]\n",
        "        if not instructions:\n",
        "            instructions = [p.get_text(strip=True) for p in soup.select(\"ol li\")]\n",
        "\n",
        "        # Nutrition (try to find a calories string)\n",
        "        nutrition = \"Not available\"\n",
        "        nutrition_section = soup.find(\"div\", class_=\"nutrition\")\n",
        "        if nutrition_section:\n",
        "            nutrition = nutrition_section.get_text(strip=True)\n",
        "        else:\n",
        "            # Try alternate\n",
        "            match = soup.find(string=lambda t: \"calories\" in t.lower())\n",
        "            if match:\n",
        "                nutrition = match.find_parent().get_text(strip=True)\n",
        "\n",
        "        recipes.append({\n",
        "            \"Title\": title,\n",
        "            \"URL\": url,\n",
        "            \"Ingredients\": \"; \".join(ingredients),\n",
        "            \"Instructions\": \" \".join(instructions),\n",
        "            \"Nutrition\": nutrition\n",
        "        })\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Failed to scrape {url}: {e}\")\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(recipes)\n",
        "df.to_excel(\"yummly_recipes.xlsx\", index=False, engine='openpyxl')\n",
        "\n",
        "print(\"✅ Data saved to yummly_recipes.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImPJz69x3-Ow",
        "outputId": "31086ad5-9665-422b-9806-8e25381e4910"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping: https://www.yummly.com/recipe/Creamy-Garlic-Chicken-2626706\n",
            "Scraping: https://www.yummly.com/recipe/Slow-Cooker-Chicken-Tikka-Masala-2335868\n",
            "Scraping: https://www.yummly.com/recipe/Easy-Honey-Garlic-Salmon-2589841\n",
            "✅ Data saved to yummly_recipes.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "base_search_url = \"https://www.yummly.com/recipes?page={page}\"\n",
        "\n",
        "def get_recipe_links(page_num):\n",
        "    url = base_search_url.format(page=page_num)\n",
        "    print(f\"Fetching search page: {url}\")\n",
        "    r = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "    # Find recipe links on the search page\n",
        "    links = []\n",
        "    for a in soup.select(\"a[href*='/recipe/']\"):\n",
        "        href = a.get(\"href\")\n",
        "        if href and href.startswith(\"/recipe/\"):\n",
        "            full_url = \"https://www.yummly.com\" + href.split('?')[0]\n",
        "            links.append(full_url)\n",
        "    # Remove duplicates\n",
        "    return list(set(links))\n",
        "\n",
        "def scrape_recipe(url):\n",
        "    print(f\"Scraping recipe: {url}\")\n",
        "    try:\n",
        "        r = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "\n",
        "        title_tag = soup.find(\"h1\")\n",
        "        title = title_tag.get_text(strip=True) if title_tag else \"N/A\"\n",
        "\n",
        "        ingredients = [i.get_text(strip=True) for i in soup.select(\"[data-testid='Ingredient']\")]\n",
        "        if not ingredients:\n",
        "            ingredients = [i.get_text(strip=True) for i in soup.select(\"li.ingredient\")]\n",
        "\n",
        "        instructions = [s.get_text(strip=True) for s in soup.select(\"[data-testid='Step']\")]\n",
        "        if not instructions:\n",
        "            instructions = [p.get_text(strip=True) for p in soup.select(\"ol li\")]\n",
        "\n",
        "        nutrition = \"Not available\"\n",
        "        nutrition_section = soup.find(\"div\", class_=\"nutrition\")\n",
        "        if nutrition_section:\n",
        "            nutrition = nutrition_section.get_text(strip=True)\n",
        "        else:\n",
        "            match = soup.find(string=lambda t: t and \"calories\" in t.lower())\n",
        "            if match:\n",
        "                nutrition = match.find_parent().get_text(strip=True)\n",
        "\n",
        "        return {\n",
        "            \"Title\": title,\n",
        "            \"URL\": url,\n",
        "            \"Ingredients\": \"; \".join(ingredients),\n",
        "            \"Instructions\": \" \".join(instructions),\n",
        "            \"Nutrition\": nutrition\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to scrape {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "all_recipes = []\n",
        "pages_to_scrape = 50  # Adjust as needed to get 200+ recipes\n",
        "\n",
        "for page in range(1, pages_to_scrape + 1):\n",
        "    recipe_links = get_recipe_links(page)\n",
        "    print(f\"Found {len(recipe_links)} recipes on page {page}\")\n",
        "\n",
        "    for link in recipe_links:\n",
        "        data = scrape_recipe(link)\n",
        "        if data:\n",
        "            all_recipes.append(data)\n",
        "        time.sleep(1)  # Be polite\n",
        "\n",
        "    if len(all_recipes) >= 200:\n",
        "        print(\"Reached 200 recipes, stopping.\")\n",
        "        break\n",
        "\n",
        "# Save all data to CSV\n",
        "df = pd.DataFrame(all_recipes)\n",
        "df.to_csv(\"yummly_200_recipes.csv\", index=False, encoding='utf-8')\n",
        "print(f\"✅ Saved {len(all_recipes)} recipes to yummly_200_recipes.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQChizS74Pmy",
        "outputId": "2a9704b7-7115-4349-bd47-41a2b037e458"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching search page: https://www.yummly.com/recipes?page=1\n",
            "Found 0 recipes on page 1\n",
            "Fetching search page: https://www.yummly.com/recipes?page=2\n",
            "Found 0 recipes on page 2\n",
            "Fetching search page: https://www.yummly.com/recipes?page=3\n",
            "Found 0 recipes on page 3\n",
            "Fetching search page: https://www.yummly.com/recipes?page=4\n",
            "Found 0 recipes on page 4\n",
            "Fetching search page: https://www.yummly.com/recipes?page=5\n",
            "Found 0 recipes on page 5\n",
            "Fetching search page: https://www.yummly.com/recipes?page=6\n",
            "Found 0 recipes on page 6\n",
            "Fetching search page: https://www.yummly.com/recipes?page=7\n",
            "Found 0 recipes on page 7\n",
            "Fetching search page: https://www.yummly.com/recipes?page=8\n",
            "Found 0 recipes on page 8\n",
            "Fetching search page: https://www.yummly.com/recipes?page=9\n",
            "Found 0 recipes on page 9\n",
            "Fetching search page: https://www.yummly.com/recipes?page=10\n",
            "Found 0 recipes on page 10\n",
            "Fetching search page: https://www.yummly.com/recipes?page=11\n",
            "Found 0 recipes on page 11\n",
            "Fetching search page: https://www.yummly.com/recipes?page=12\n",
            "Found 0 recipes on page 12\n",
            "Fetching search page: https://www.yummly.com/recipes?page=13\n",
            "Found 0 recipes on page 13\n",
            "Fetching search page: https://www.yummly.com/recipes?page=14\n",
            "Found 0 recipes on page 14\n",
            "Fetching search page: https://www.yummly.com/recipes?page=15\n",
            "Found 0 recipes on page 15\n",
            "Fetching search page: https://www.yummly.com/recipes?page=16\n",
            "Found 0 recipes on page 16\n",
            "Fetching search page: https://www.yummly.com/recipes?page=17\n",
            "Found 0 recipes on page 17\n",
            "Fetching search page: https://www.yummly.com/recipes?page=18\n",
            "Found 0 recipes on page 18\n",
            "Fetching search page: https://www.yummly.com/recipes?page=19\n",
            "Found 0 recipes on page 19\n",
            "Fetching search page: https://www.yummly.com/recipes?page=20\n",
            "Found 0 recipes on page 20\n",
            "Fetching search page: https://www.yummly.com/recipes?page=21\n",
            "Found 0 recipes on page 21\n",
            "Fetching search page: https://www.yummly.com/recipes?page=22\n",
            "Found 0 recipes on page 22\n",
            "Fetching search page: https://www.yummly.com/recipes?page=23\n",
            "Found 0 recipes on page 23\n",
            "Fetching search page: https://www.yummly.com/recipes?page=24\n",
            "Found 0 recipes on page 24\n",
            "Fetching search page: https://www.yummly.com/recipes?page=25\n",
            "Found 0 recipes on page 25\n",
            "Fetching search page: https://www.yummly.com/recipes?page=26\n",
            "Found 0 recipes on page 26\n",
            "Fetching search page: https://www.yummly.com/recipes?page=27\n",
            "Found 0 recipes on page 27\n",
            "Fetching search page: https://www.yummly.com/recipes?page=28\n",
            "Found 0 recipes on page 28\n",
            "Fetching search page: https://www.yummly.com/recipes?page=29\n",
            "Found 0 recipes on page 29\n",
            "Fetching search page: https://www.yummly.com/recipes?page=30\n",
            "Found 0 recipes on page 30\n",
            "Fetching search page: https://www.yummly.com/recipes?page=31\n",
            "Found 0 recipes on page 31\n",
            "Fetching search page: https://www.yummly.com/recipes?page=32\n",
            "Found 0 recipes on page 32\n",
            "Fetching search page: https://www.yummly.com/recipes?page=33\n",
            "Found 0 recipes on page 33\n",
            "Fetching search page: https://www.yummly.com/recipes?page=34\n",
            "Found 0 recipes on page 34\n",
            "Fetching search page: https://www.yummly.com/recipes?page=35\n",
            "Found 0 recipes on page 35\n",
            "Fetching search page: https://www.yummly.com/recipes?page=36\n",
            "Found 0 recipes on page 36\n",
            "Fetching search page: https://www.yummly.com/recipes?page=37\n",
            "Found 0 recipes on page 37\n",
            "Fetching search page: https://www.yummly.com/recipes?page=38\n",
            "Found 0 recipes on page 38\n",
            "Fetching search page: https://www.yummly.com/recipes?page=39\n",
            "Found 0 recipes on page 39\n",
            "Fetching search page: https://www.yummly.com/recipes?page=40\n",
            "Found 0 recipes on page 40\n",
            "Fetching search page: https://www.yummly.com/recipes?page=41\n",
            "Found 0 recipes on page 41\n",
            "Fetching search page: https://www.yummly.com/recipes?page=42\n",
            "Found 0 recipes on page 42\n",
            "Fetching search page: https://www.yummly.com/recipes?page=43\n",
            "Found 0 recipes on page 43\n",
            "Fetching search page: https://www.yummly.com/recipes?page=44\n",
            "Found 0 recipes on page 44\n",
            "Fetching search page: https://www.yummly.com/recipes?page=45\n",
            "Found 0 recipes on page 45\n",
            "Fetching search page: https://www.yummly.com/recipes?page=46\n",
            "Found 0 recipes on page 46\n",
            "Fetching search page: https://www.yummly.com/recipes?page=47\n",
            "Found 0 recipes on page 47\n",
            "Fetching search page: https://www.yummly.com/recipes?page=48\n",
            "Found 0 recipes on page 48\n",
            "Fetching search page: https://www.yummly.com/recipes?page=49\n",
            "Found 0 recipes on page 49\n",
            "Fetching search page: https://www.yummly.com/recipes?page=50\n",
            "Found 0 recipes on page 50\n",
            "✅ Saved 0 recipes to yummly_200_recipes.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mablgTkI6bO9"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}
