# -*- coding: utf-8 -*-
"""Mistral_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MUgOaabBsJgRHMceP84BKnI-S_Fp4hDX
"""

!pip uninstall -y torch torchvision torchaudio bitsandbytes
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install bitsandbytes
!pip install sentence-transformers faiss-cpu transformers accelerate --quiet

!pip install -q transformers accelerate bitsandbytes sentence-transformers faiss-cpu

import pandas as pd
import numpy as np
import re
import torch
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import faiss
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

recipes_df = pd.read_csv("/content/70000_recipes_nutrients_cleaned_final.csv")
health_df = pd.read_csv("/content/health_age_data_70000_synthetic.csv")

def clean_ingredients(raw_ingredients):
    items = [re.sub(r"[^a-zA-Z ]", "", x).strip().lower() for x in str(raw_ingredients).split(',')]
    items = [i for i in items if len(i) > 1 and not i.isdigit() and i != 'salt freshly ground pepper']

    stop_words = {"grated", "cut", "chopped", "sliced", "crushed", "peeled",
                  "washed", "dried", "fresh", "lowfat", "cup", "teaspoon",
                  "tablespoon", "ml", "g", "piece", "pieces", "bunches"}

    cleaned = []
    for item in items:
        tokens = [word for word in item.split() if word not in stop_words]
        cleaned_item = ' '.join(tokens)
        if cleaned_item and cleaned_item not in cleaned:
            cleaned.append(cleaned_item)

    return ', '.join(sorted(set(cleaned)))

health_rules = {
    "Diabetes": {
        "avoid": ["sugar", "white rice", "ghee", "salt"],
        "recommend": ["olive oil", "turmeric", "cinnamon", "fiber-rich foods"]
    },
    "Hypertension": {
        "avoid": ["salt", "cheese", "processed meat"],
        "recommend": ["spinach", "garlic", "low-sodium spices"]
    },
    "Heart Disease": {
        "avoid": ["butter", "saturated fat", "excess salt"],
        "recommend": ["omega-3s", "olive oil", "leafy greens"]
    }
}

def get_user_nutrient_goals(age):
    age_range = health_df[(health_df["Ages"] >= age - 3) & (health_df["Ages"] <= age + 3)]
    if age_range.empty:
        return {}
    return age_range.iloc[0].to_dict()

embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
recipe_names = recipes_df["recipe_name"].astype(str).tolist()

batch_size = 512
recipe_embeddings = []

for i in tqdm(range(0, len(recipe_names), batch_size), desc="ðŸ” Embedding Batches"):
    batch = recipe_names[i:i+batch_size]
    batch_embeddings = embedding_model.encode(batch, convert_to_numpy=True)
    recipe_embeddings.append(batch_embeddings)

recipe_embeddings = np.vstack(recipe_embeddings)
index = faiss.IndexFlatL2(recipe_embeddings.shape[1])
index.add(recipe_embeddings)

#gpu
from huggingface_hub import login
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

# Authenticate
login("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")

# Configure quantization for GPU
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4"
)

model_name = "mistralai/Mistral-7B-Instruct-v0.1"

# Load tokenizer and model onto GPU
tokenizer = AutoTokenizer.from_pretrained(model_name, token="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")
mistral_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    token="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"
)

def get_similar_dishes(dish_name, top_k=3):
    query_vec = embedding_model.encode([dish_name], convert_to_numpy=True)
    D, I = index.search(query_vec, top_k)
    return [recipe_names[i] for i in I[0] if i < len(recipe_names)]

def llm(prompt, max_tokens=768):
    inputs = tokenizer(prompt, return_tensors="pt").to(mistral_model.device)
    outputs = mistral_model.generate(
        **inputs,
        max_new_tokens=max_tokens,
        temperature=0.9,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# âœ… Full hallucination-free LLM pipeline

# Approved healthy additions for diabetes (can be customized)
approved_additions = [
    "turmeric", "olive oil", "cinnamon", "spinach", "broccoli", "kale",
    "bell peppers", "mushrooms", "zucchini", "flaxseeds", "chia seeds", "whole grain pasta"
]

# ðŸš€ Function to build hallucination-safe prompt
def build_strict_prompt(dish_name, age, disease, ingredients, avoid, recommend, similar_dishes, targets):
    cleaned = clean_ingredients(ingredients)
    cleaned_list = [i.strip() for i in cleaned.split(',') if i.strip()]

    avoid_str = ', '.join(set(avoid))
    recommend_str = ', '.join(set(recommend))
    similar_str = ', '.join(set(similar_dishes))

    approved_add_str = ', '.join(approved_additions)
    cleaned_bullets = '\n'.join(f"- {i}" for i in cleaned_list)

    prompt = f"""
You are a professional nutritionist reviewing a recipe for a {age}-year-old person with {disease}.

Dish Name: {dish_name}
Cleaned Ingredients:
{cleaned_bullets}

Daily Nutrient Needs:
- Calories: {targets.get('Daily Calorie Target', 'N/A')}
- Protein: {targets.get('Protein', 'N/A')}g
- Carbs: {targets.get('Carbohydrates', 'N/A')}g
- Fat: {targets.get('Fat', 'N/A')}g
- Fiber: {targets.get('Fiber', 'N/A')}g

\u26a0\ufe0f STRICT RULES:
- You may only REMOVE ingredients from this list above.
- You may only ADD ingredients from this list: {approved_add_str}.
- Do NOT mention any ingredient not in these lists (e.g., ghee, white rice, sugar).
- Do NOT invent ingredients or swaps that don't follow the rule above.

\u2705 TASK:
Now write a single, friendly paragraph explaining:
1. What the full cleaned ingredients are.
2. What you are removing and why (only if it exists in the list).
3. What you are adding and why (only from the approved list).
4. How these changes improve the dish for a {age}-year-old with {disease}.

Write in a professional, nutritionist tone. Do not use bullet points.
"""
    return prompt.strip()

# ðŸš€ Main wrapper function for hallucination-safe generation
def generate_hallucination_free_output(dish_name, age, disease, recipes_df, health_df, health_rules, get_similar_dishes, clean_ingredients, get_user_nutrient_goals, llm):
    selected_dish = get_similar_dishes(dish_name)[0]
    row = recipes_df[recipes_df["recipe_name"].str.lower() == selected_dish.lower()]

    if row.empty:
        return f"âŒ Could not find ingredients for {selected_dish}"

    raw_ingredients = row.iloc[0]["ingredients"]
    cleaned = clean_ingredients(raw_ingredients)

    disease_lower = disease.lower()
    matched_key = [d for d in health_rules if d.lower() == disease_lower][0]
    avoid = health_rules[matched_key]["avoid"]
    recommend = health_rules[matched_key]["recommend"]
    targets = get_user_nutrient_goals(age)

    prompt = build_strict_prompt(
        dish_name=selected_dish,
        age=age,
        disease=disease,
        ingredients=raw_ingredients,
        avoid=avoid,
        recommend=recommend,
        similar_dishes=get_similar_dishes(dish_name),
        targets=targets
    )

    print("\n--- Prompt Sent to LLM ---\n")
    print(prompt)

    response = llm(prompt)

    print("\n--- LLM Output ---\n")
    print(response)

    return response

# ðŸ¥˜ Sample input values
dish_name = "butter chicken"
age = 30
disease = "diabetes"

# âœ… Final call to run the full process and print output
output = generate_hallucination_free_output(
    dish_name=dish_name,
    age=age,
    disease=disease,
    recipes_df=recipes_df,
    health_df=health_df,
    health_rules=health_rules,
    get_similar_dishes=get_similar_dishes,
    clean_ingredients=clean_ingredients,
    get_user_nutrient_goals=get_user_nutrient_goals,
    llm=llm  # Your Mistral-based inference function
)

print("\nâœ… Final Output:\n")
print(output)

"""**INTERPRATATION**"""

!pip install cleanlab-tlm

import os

# Set your Cleanlab API key
os.environ["CLEANLAB_TLM_API_KEY"] = "0d971ca876c443819414f3b8b8248a25"

from cleanlab_tlm import TLM

# Initialize the model with explanation logging
tlm = TLM(options={"log": ["explanation"]})

def generate_hallucination_free_output(
    dish_name, age, disease, recipes_df, health_df, health_rules,
    get_similar_dishes, clean_ingredients, get_user_nutrient_goals, llm
):
    selected_dish = get_similar_dishes(dish_name)[0]
    row = recipes_df[recipes_df["recipe_name"].str.lower() == selected_dish.lower()]

    if row.empty:
        return f"âŒ Could not find ingredients for {selected_dish}", ""

    raw_ingredients = row.iloc[0]["ingredients"]
    cleaned = clean_ingredients(raw_ingredients)

    disease_lower = disease.lower()
    matched_key = [d for d in health_rules if d.lower() == disease_lower][0]
    avoid = health_rules[matched_key]["avoid"]
    recommend = health_rules[matched_key]["recommend"]
    targets = get_user_nutrient_goals(age)

    prompt = build_strict_prompt(
        dish_name=selected_dish,
        age=age,
        disease=disease,
        ingredients=raw_ingredients,
        avoid=avoid,
        recommend=recommend,
        similar_dishes=get_similar_dishes(dish_name),
        targets=targets
    )

    print("\n--- Prompt Sent to LLM ---\n")
    print(prompt)

    response = llm(prompt)

    print("\n--- LLM Output ---\n")
    print(response)

    return response, prompt

output, prompt = generate_hallucination_free_output(
    dish_name=dish_name,
    age=age,
    disease=disease,
    recipes_df=recipes_df,
    health_df=health_df,
    health_rules=health_rules,
    get_similar_dishes=get_similar_dishes,
    clean_ingredients=clean_ingredients,
    get_user_nutrient_goals=get_user_nutrient_goals,
    llm=llm
)

from cleanlab_tlm import TLM
import os

# Step 1: Set your API key
os.environ["CLEANLAB_TLM_API_KEY"] = "0d971ca876c443819414f3b8b8248a25"

# Step 2: Initialize TLM
tlm = TLM(options={"log": ["explanation"]})

# Step 3: Use the output (LLM response) and prompt you already generated
score_data = tlm.get_trustworthiness_score(prompt=prompt, response=output)

# Step 4: Extract trust score and explanation only
trust_score = score_data.get("trustworthiness_score", None)
explanation = score_data.get("explanation", "No explanation provided.")

# Step 5: Display results
print("ðŸ”¢ Trust Score:", trust_score)
print("ðŸ“ Explanation:", explanation)